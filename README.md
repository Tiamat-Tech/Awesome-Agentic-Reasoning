# Awesome Agentic Reasoning (Survey 2025) ‚ú®

<div align="center">

![Planning](https://img.shields.io/badge/Planning-172-7b61ff) ![Tool Use](https://img.shields.io/badge/Tool_Use-10-4a90e2) ![RAG](https://img.shields.io/badge/RAG-39-3fb950) ![Execution](https://img.shields.io/badge/Execution-6-ffb000) ![Feedback](https://img.shields.io/badge/Feedback-4-f66a0a) ![Memory](https://img.shields.io/badge/Memory-62-a371f7) ![Multi-Agent](https://img.shields.io/badge/Multi--Agent-124-0fb) ![Apps & Benchmarks](https://img.shields.io/badge/Apps_%26_Benchmarks-10-999999)

<br/>

[![Made for Agents](https://img.shields.io/badge/Made%20for-Agents-red)]() [![Updated](https://img.shields.io/badge/Updated-2025--08--30-green)]()

</div>

<img width="1536" height="1024" alt="6319fe6f-11f6-4872-913f-5fef14d07783" src="https://github.com/user-attachments/assets/3d7e0069-b1d4-46c5-b411-93b25e4e4c67" />


---

## Table of Contents
- üó∫Ô∏è [Planning](#planning)
- üîß [Tool Use & Integration](#tool-use-and-integration)
- üîç [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
- üéÆ [Execution](#execution)
- üîÑ [Feedback & Reflection](#feedback-and-reflection)
- üóÉÔ∏è [Memory](#memory)
- ü§ù [Multi-Agent Collaboration](#multi-agent-collaboration)
- üåê [Applications & Benchmarks](#applications-and-benchmarks)



## Planning

| ID | Paper / Resource | Links |
|---:|---|---|
| [25](#ref-25) | Enhancing llm-based agents via global planning and hierarchical execution | [arXiv:2504.16563](https://arxiv.org/abs/2504.16563) |
| [28](#ref-28) | Research: Learning to reason with search for llms via reinforcement learning, 2025 | [Link](https://arxiv.org/abs/2503.19470) |
| [29](#ref-29) | Learning to reason with search for llms via reinforcement learning | [arXiv:2503.19470](https://arxiv.org/abs/2503.19470) |
| [30](#ref-30) | In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:... | [Link](https://doi.org/10.18653/v1/2024.acl-long.515) |
| [31](#ref-31) | Scaling autonomous agents via automatic reward modeling and planning | [arXiv:2502.12130](https://arxiv.org/abs/2502.12130) |
| [32](#ref-32) | Broaden your scope! efficient multi-turn conversation planning for llms with semantic space | [arXiv:2503.11586](https://arxiv.org/abs/2503.11586) |
| [33](#ref-33) | In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 14777‚Äì14790, Singapore, ... | [Link](https://aclanthology.org/2023.findings-emnlp.985/) |
| [34](#ref-34) | When is tree search useful for llm planning? it depends on the discriminator | [arXiv:2402.10890](https://arxiv.org/abs/2402.10890) |
| [35](#ref-35) | Robohorizon: An llm-assisted multi-view world model for long-horizon robotic manipulation | [arXiv:2501.06605](https://arxiv.org/abs/2501.06605) |
| [36](#ref-36) | Seeclick: Harnessing gui grounding for advanced visual gui agents | [arXiv:2401.10935](https://arxiv.org/abs/2401.10935) |
| [37](#ref-37) | Mem0: Building production- ready ai agents with scalable long-term memory | [arXiv:2504.19413](https://arxiv.org/abs/2504.19413) |
| [40](#ref-40) | Hierarchical planning for complex tasks with knowledge graph-rag and symbolic verification | [arXiv:2504.04578](https://arxiv.org/abs/2504.04578) |
| [51](#ref-51) | Re-rest: Reflection-reinforced self-training for language agents | [arXiv:2406.01495](https://arxiv.org/abs/2406.01495) |
| [52](#ref-52) | Palm-e: An embodied multimodal language model | ‚Äî |
| [53](#ref-53) | Perltqa: A personal long-term memory dataset for memory classification, retrieval, and fusion in question answering | ‚Äî |
| [56](#ref-56) | Plan-and-act: Improving planning of agents for long-horizon tasks | [arXiv:2503.09572](https://arxiv.org/abs/2503.09572) |
| [57](#ref-57) | Webevolver: Enhancing web agent self-improvement with coevolving world model | [arXiv:2504.21024](https://arxiv.org/abs/2504.21024) |
| [58](#ref-58) | Retool: Reinforcement learning for strategic tool use in llms | [arXiv:2504.11536](https://arxiv.org/abs/2504.11536) |
| [63](#ref-63) | Flip: Flow-centric generative planning as general-purpose manipulation world model | [arXiv:2412.08261](https://arxiv.org/abs/2412.08261) |
| [64](#ref-64) | Realm-bench: A real-world planning benchmark for llms and multi-agent systems | [arXiv:2502.18836](https://arxiv.org/abs/2502.18836) |
| [65](#ref-65) | URL https://www.arxiv.org/abs/2303.17491 . | [arXiv:2303.17491](https://arxiv.org/abs/2303.17491)<br/>[Link](https://www.arxiv.org/abs/2303.17491) |
| [66](#ref-66) | Pathfinder: Guided search over multi-step reasoning paths | [arXiv:2312.05180](https://arxiv.org/abs/2312.05180) |
| [67](#ref-67) | Is your llm secretly a world model of the internet? model-based planning for web agents | [arXiv:2411.06559](https://arxiv.org/abs/2411.06559) |
| [68](#ref-68) | Leveraging pre-trained large language models to construct and utilize world models for model-based task planning | ‚Äî |
| [69](#ref-69) | Deeprag: Thinking to retrieve step by step for large language models | [arXiv:2502.01142](https://arxiv.org/abs/2502.01142) |
| [70](#ref-70) | Intelligent virtual assistants with llm-based process automation | [arXiv:2312.06677](https://arxiv.org/abs/2312.06677) |
| [71](#ref-71) | Hypertree planning: Enhancing llm reasoning via hierarchical thinking | [arXiv:2505.02322](https://arxiv.org/abs/2505.02322) |
| [72](#ref-72) | Codenav: Beyond tool-use to using real-world codebases with llm agents | [arXiv:2406.12276](https://arxiv.org/abs/2406.12276) |
| [76](#ref-76) | In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: An... | [Link](http://papers.nips.cc/paper_files/paper/2023/hash/) |
| [77](#ref-77) | LLM reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models | [arXiv:2404.05221](https://arxiv.org/abs/2404.05221) |
| [78](#ref-78) | Planning anything with rigor: General-purpose zero-shot planning with llm-based formalized programming | [arXiv:2410.12112](https://arxiv.org/abs/2410.12112) |
| [79](#ref-79) | Acpbench: Reasoning about action, change, and planning | [arXiv:2410.05669](https://arxiv.org/abs/2410.05669)<br/>[Link](https://www.arxiv.org/abs/2410.05669) |
| [80](#ref-80) | Enhancing llm reasoning with multi-path collaborative reactive and reflection agents | [arXiv:2501.00430](https://arxiv.org/abs/2501.00430) |
| [81](#ref-81) | Webvoyager: Building an end-to-end web agent with large multimodal models | [arXiv:2401.13919](https://arxiv.org/abs/2401.13919) |
| [82](#ref-82) | In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management , pages 829‚Äì838, 2024. | ‚Äî |
| [83](#ref-83) | Multi-if: Benchmarking llms on multi-turn and multilingual instructions following | [arXiv:2410.15553](https://arxiv.org/abs/2410.15553) |
| [84](#ref-84) | URL https://www.arxiv.org/abs/2103.04000 . | [arXiv:2103.04000](https://arxiv.org/abs/2103.04000)<br/>[Link](https://www.arxiv.org/abs/2103.04000) |
| [85](#ref-85) | In The Twelfth International Conference on Learning Representations , 2024 | [Link](https://openreview.net/forum?id=) |
| [93](#ref-93) | Inner monologue: Embodied reasoning through planning with language models | [arXiv:2207.05608](https://arxiv.org/abs/2207.05608) |
| [94](#ref-94) | A survey on retrieval-augmented text generation for large language models | [arXiv:2404.10981](https://arxiv.org/abs/2404.10981) |
| [95](#ref-95) | In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 | [Link](https://openreview.net/forum?id=R0c2qtalgG) |
| [96](#ref-96) | In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol... | [Link](https://aclanthology.org/2023.acl-short.130/) |
| [97](#ref-97) | In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 9628‚Äì9635 | ‚Äî |
| [98](#ref-98) | In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 9684‚Äì9690 | ‚Äî |
| [99](#ref-99) | Intersectionzoo: Eco-driving for benchmarking multi-agent contextual reinforcement learning | [arXiv:2410.15221](https://arxiv.org/abs/2410.15221) |
| [104](#ref-104) | Search-r1: Training llms to reason and leverage search engines with reinforcement learning | [arXiv:2503.09516](https://arxiv.org/abs/2503.09516) |
| [105](#ref-105) | Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025 | [Link](https://arxiv.org/abs/2503.09516) |
| [106](#ref-106) | Visualwebarena: Evaluating multimodal agents on realistic visual web tasks | [arXiv:2401.13649](https://arxiv.org/abs/2401.13649)<br/>[Link](https://www.arxiv.org/abs/2401) |
| [107](#ref-107) | Gta: A benchmark for general tool agents | [arXiv:2407.08713](https://arxiv.org/abs/2407.08713)<br/>[Link](https://www.arxiv.org/abs/2407) |
| [108](#ref-108) | Lab-bench: Measuring capabilities of language models for biology research | [arXiv:2407.10362](https://arxiv.org/abs/2407.10362)<br/>[Link](https://www.arxiv) |
| [111](#ref-111) | Agentic reasoning and tool integration for llms via reinforcement learning | [arXiv:2505.01441v1](https://arxiv.org/abs/2505.01441v1)<br/>[Link](https://www.arxiv.org/) |
| [115](#ref-115) | Cultivating multimodal intelligence: Interpretive reasoning and agentic rag approaches to dermatological diagnosis | [arXiv:2507.05520v1](https://arxiv.org/abs/2507.05520v1)<br/>[Link](https://www.arxiv.org/abs/2507.05520v1) |
| [117](#ref-117) | Thought of search: Planning with language models through the lens of efficiency | ‚Äî |
| [118](#ref-118) | Nestful: A benchmark for evaluating llms on nested sequences of api calls | [arXiv:2409.03797](https://arxiv.org/abs/2409.03797) |
| [119](#ref-119) | Understanding the effects of rlhf on llm generalisation and diversity | [arXiv:2310.06452](https://arxiv.org/abs/2310.06452) |
| [120](#ref-120) | Tree search for language model agents | [arXiv:2407.01476](https://arxiv.org/abs/2407.01476) |
| [121](#ref-121) | Latent plan transformer for trajectory abstraction: Planning as latent space inference | ‚Äî |
| [122](#ref-122) | Realtalk: A 21-day real-world dataset for long-term conversation | [arXiv:2502.13270](https://arxiv.org/abs/2502.13270) |
| [123](#ref-123) | Agent-g: An agentic framework for graph retrieval augmented generation. | ‚Äî |
| [124](#ref-124) | Scalable evaluation of multi-agent reinforcement learning with melting pot | ‚Äî |
| [129](#ref-129) | Agent-oriented planning in multi-agent systems | [arXiv:2410.02189](https://arxiv.org/abs/2410.02189) |
| [136](#ref-136) | Autoflow: Automated workflow generation for large language model agents | [Link](https://api.semanticscholar.org/CorpusID:271270428) |
| [137](#ref-137) | Memos: An operating system for memory-augmented generation (mag) in large language models | [Link](https://api.semanticscholar.org/CorpusID:278960153) |
| [142](#ref-142) | Avalonbench: Evaluating llms playing the game of avalon, 2023 | [Link](https://arxiv) |
| [143](#ref-143) | Sleep- time compute: Beyond inference scaling at test-time | [Link](https://api) |
| [144](#ref-144) | In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 19498‚Äì19508, 2025. | ‚Äî |
| [145](#ref-145) | In The Twelfth International Conference on Learning Representations , 2023 | ‚Äî |
| [146](#ref-146) | In Advances in Neural Information Processing Systems , volume 37, 2024. | ‚Äî |
| [147](#ref-147) | Moralise: A structured benchmark for moral alignment in visual language models, 2025. | ‚Äî |
| [148](#ref-148) | LLM+P: Empowering large language models with optimal planning proficiency | [arXiv:2304.11477](https://arxiv.org/abs/2304.11477) |
| [149](#ref-149) | Multimodal large language models for inverse molecular design with retrosynthetic planning | [arXiv:2410.04223](https://arxiv.org/abs/2410.04223) |
| [150](#ref-150) | URL https://github.com/jerryjliu/llama_index . | [Code](https://github.com/jerryjliu/llama_index) |
| [151](#ref-151) | Logic query of thoughts: Guiding large language models to answer complex logic queries with knowledge graphs, 2024. | ‚Äî |
| [155](#ref-155) | In The Thirteenth International Conference on Learning Representations , 2025 | [Link](https://openreview.net/forum?id=) |
| [156](#ref-156) | Topological augmentation for class-imbalanced node classification, 2023. | ‚Äî |
| [157](#ref-157) | In Proceedings of the 41st International Conference on Machine Learning , 2024. | ‚Äî |
| [158](#ref-158) | In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 2014‚Äì2025, 2024. | ‚Äî |
| [159](#ref-159) | In Proceedings of the 42nd International Conference on Machine Learning , 2025. | ‚Äî |
| [160](#ref-160) | Continual reinforcement learning by planning with online world models | [arXiv:2507.09177](https://arxiv.org/abs/2507.09177) |
| [161](#ref-161) | Journal of Machine Learning Research , 25(330):1‚Äì57, 2024. | ‚Äî |
| [162](#ref-162) | Large language model guided tree-of-thought | [arXiv:2305.08291](https://arxiv.org/abs/2305.08291) |
| [163](#ref-163) | Teamcraft: A benchmark for multi-modal multi-agent systems in minecraft | [arXiv:2412.05255](https://arxiv.org/abs/2412.05255) |
| [169](#ref-169) | In Forty-second International Conference on Machine Learning , 2025. | ‚Äî |
| [170](#ref-170) | In 2024 2nd International Conference on Foundation and Large Language Models (FLLM) , pages 392‚Äì397 | ‚Äî |
| [171](#ref-171) | Non-myopic generation of language models for reasoning and planning | [arXiv:2410.17195](https://arxiv.org/abs/2410.17195) |
| [172](#ref-172) | Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning | ‚Äî |
| [179](#ref-179) | Generalizable motion planning via operator learning | [arXiv:2410.17547](https://arxiv.org/abs/2410.17547) |
| [180](#ref-180) | Llm-a*: Large language model enhanced incremental heuristic search on path planning | [arXiv:2407.02511](https://arxiv.org/abs/2407.02511) |
| [181](#ref-181) | Telograf: Temporal logic planning via graph-encoded flow matching | [arXiv:2505.00562](https://arxiv.org/abs/2505.00562) |
| [182](#ref-182) | Agent-as-a-judge: Evaluate agents with agents | [arXiv:2410.10934](https://arxiv.org/abs/2410.10934)<br/>[Link](https://www.arxiv.org/abs/2410.10934) |
| [183](#ref-183) | Understanding the weakness of large language model agents within a complex android environment | [arXiv:2402.06596v1](https://arxiv.org/abs/2402.06596v1)<br/>[Link](https://www.arxiv.org/abs/2402.06596v1) |
| [184](#ref-184) | Ret-llm: Towards a general read-write memory for large language models | [Link](https://api.semanticscholar) |
| [190](#ref-190) | In 2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge) , pages 156‚Äì167 | ‚Äî |
| [194](#ref-194) | Physics-informed temporal difference metric learning for robot motion planning | [arXiv:2505.05691](https://arxiv.org/abs/2505.05691) |
| [195](#ref-195) | Tree-of-code: A hybrid approach for robust complex task planning and execution | [arXiv:2412.14212](https://arxiv.org/abs/2412.14212) |
| [213](#ref-213) | Measuring and narrowing the compositionality gap in language models | [arXiv:2210.03350](https://arxiv.org/abs/2210.03350) |
| [214](#ref-214) | Agent q: Advanced reasoning and learning for autonomous ai agents | [arXiv:2408.07199](https://arxiv.org/abs/2408.07199) |
| [215](#ref-215) | In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 6922‚Äì6939, Singapore, De... | [Link](https://aclanthology.org/2023.findings-emnlp.462/) |
| [216](#ref-216) | Toolrl: Reward is all tool learning needs | [arXiv:2504.13958](https://arxiv.org/abs/2504.13958) |
| [217](#ref-217) | In The Thirteenth International Conference on Learning Representations , 2025. | ‚Äî |
| [218](#ref-218) | Mlagentbench: Evaluating language agents on machine learning experimentation | [arXiv:2310.03302](https://arxiv.org/abs/2310.03302)<br/>[Link](https://www.arxiv.org/abs/2310) |
| [219](#ref-219) | Advances in Neural Information Processing Systems , 37:114843‚Äì114871, 2024. | ‚Äî |
| [220](#ref-220) | Toolllm: Facilitating large language models to master 16000+ real-world apis | [arXiv:2307.16789](https://arxiv.org/abs/2307.16789) |
| [221](#ref-221) | In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 | [Link](https://openreview.net/forum?id=dHng2O0Jjr) |
| [222](#ref-222) | Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Y... | [Link](https://doi.org/10.1145/3704435) |
| [223](#ref-223) | Gradient compressed sensing: A query-efficient gradient estimator for high-dimensional zeroth-order optimization | ‚Äî |
| [224](#ref-224) | In Advances in Neural Information Processing Systems , volume 35, pages 25531‚Äì25546, 2022. | ‚Äî |
| [225](#ref-225) | In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 1978‚Äì1988, 2023. | ‚Äî |
| [226](#ref-226) | TUCKET: A tensor time series data structure for efficient and accurate factor analysis over time ranges | ‚Äî |
| [227](#ref-227) | Saffron-1: Safety inference scaling, 2025. | ‚Äî |
| [228](#ref-228) | In 13th International Conference on Learning Representations , 2025. | ‚Äî |
| [229](#ref-229) | How efficient is LLM-generated code? A rigorous & high-standard benchmark | ‚Äî |
| [230](#ref-230) | Frontiers of Computer Science , 19(8):198343, 2025. | ‚Äî |
| [231](#ref-231) | Marco: A multi-agent system for optimizing hpc code generation using large language models | [arXiv:2505.03906](https://arxiv.org/abs/2505.03906) |
| [234](#ref-234) | Zep: A temporal knowledge graph architecture for agent memory | [Link](https://api.semanticscholar) |
| [236](#ref-236) | Pommerman: A multi-agent playground | [arXiv:1809.07124](https://arxiv.org/abs/1809.07124) |
| [238](#ref-238) | Advances in Neural Information Processing Systems , 37:65765‚Äì65790, 2024 | ‚Äî |
| [239](#ref-239) | Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint  , 2022 | [arXiv:2203.07540](https://arxiv.org/abs/2203.07540)<br/>[Link](https://www.arxiv.org/abs/) |
| [240](#ref-240) | x: Learning to balance fast and slow planning with language models | [arXiv:2407.14414](https://arxiv.org/abs/2407.14414) |
| [241](#ref-241) | Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments | [arXiv:2405.07960](https://arxiv.org/abs/2405.07960)<br/>[Link](https://www.arxiv.org/abs/2405.07960) |
| [242](#ref-242) | The starcraft multi-agent challenge | [arXiv:1902.04043](https://arxiv.org/abs/1902.04043) |
| [245](#ref-245) | Algorithm of thoughts: Enhancing exploration of ideas in large language models | [arXiv:2308.10379](https://arxiv.org/abs/2308.10379) |
| [246](#ref-246) | Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action | ‚Äî |
| [247](#ref-247) | Medbrowsecomp: Benchmarking medical deep research and computer use | [arXiv:2505.14963](https://arxiv.org/abs/2505.14963)<br/>[Link](https://www.arxiv.org/abs/2505.14963) |
| [248](#ref-248) | In The Thirteenth International Conference on Learning Representations , 2025. | ‚Äî |
| [249](#ref-249) | Advances in Neural Information Processing Systems , 36: 38154‚Äì38180, 2023. | ‚Äî |
| [250](#ref-250) | Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023 | [Link](https://arxiv.org/abs/2303.17580) |
| [251](#ref-251) | Gear: Graph-enhanced agent for retrieval-augmented generation | [arXiv:2412.18431](https://arxiv.org/abs/2412.18431) |
| [252](#ref-252) | Learning to use tools via cooperative and interactive agents | [arXiv:2403.03031](https://arxiv.org/abs/2403.03031) |
| [253](#ref-253) | Tool learning in the wild: Empowering language models as automatic tool agents | ‚Äî |
| [254](#ref-254) | Retrieval models aren‚Äôt tool-savvy: Benchmarking tool retrieval for large language models | [Link](https://doi.org/10.48550/arXiv.2503) |
| [255](#ref-255) | Advances in Neural Information Processing Systems , 36:8634‚Äì8652, 2023. | ‚Äî |
| [256](#ref-256) | Reflexion: language agents with verbal reinforcement learning | [Link](https://api.semanticscholar.org/CorpusID:258833055) |
| [257](#ref-257) | Webarena: A realistic web environment for building autonomous agents | [arXiv:2307.13854](https://arxiv.org/abs/2307.13854)<br/>[Link](https://www.arxiv.org/abs/2307) |
| [260](#ref-260) | Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms | [arXiv:2501.17399](https://arxiv.org/abs/2501.17399) |
| [261](#ref-261) | Pogema: A benchmark platform for cooperative multi-agent pathfinding | [arXiv:2407.14931](https://arxiv.org/abs/2407.14931) |
| [263](#ref-263) | Restgpt: Connecting large language models with real-world restful apis | [arXiv:2306.06624](https://arxiv.org/abs/2306.06624) |
| [264](#ref-264) | InProceedings of the AAAI conference on artificial intelligence , volume 34, pages 7253‚Äì7260, 2020. | ‚Äî |
| [283](#ref-283) | On the planning abilities of large language models: A critical investigation | ‚Äî |
| [284](#ref-284) | Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world | ‚Äî |
| [288](#ref-288) | In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 2256‚Äì2268, 2023. | ‚Äî |
| [289](#ref-289) | Adawm: Adaptive world model based planning for autonomous driving | [arXiv:2501.13072](https://arxiv.org/abs/2501.13072) |
| [290](#ref-290) | In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 2983‚Äì2986, 2024 | ‚Äî |
| [291](#ref-291) | Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, Neu... | [Link](http://papers.nips.cc/paper_files/paper/2024/hash/) |
| [292](#ref-292) | Plan-and- solve prompting: Improving zero-shot chain-of-thought reasoning by large language models | [arXiv:2305.04091](https://arxiv.org/abs/2305.04091) |
| [293](#ref-293) | Goplan: Goal-conditioned offline reinforcement learning by planning with learned models | [arXiv:2310.20025](https://arxiv.org/abs/2310.20025) |
| [294](#ref-294) | Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng | [Link](https://openreview.net/forum?id=6guG2OlXsr) |
| [295](#ref-295) | Battleagentbench: A benchmark for evalu- ating cooperation and competition capabilities of language models in multi-agent systems | [arXiv:2408.15971](https://arxiv.org/abs/2408.15971) |
| [307](#ref-307) | Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records | [arXiv:2401.07128](https://arxiv.org/abs/2401.07128)<br/>[Link](https://www.arxiv.org/abs/2401) |
| [308](#ref-308) | Longmemeval: Bench- marking chat assistants on long-term interactive memory | ‚Äî |
| [319](#ref-319) | Search-o1: Agentic search-enhanced large reasoning models | [arXiv:2501.05366](https://arxiv.org/abs/2501.05366)<br/>[Link](https://www.arxiv.org/abs/2501.05366) |
| [320](#ref-320) | Latent diffusion planning for imitation learning | [arXiv:2504.16925](https://arxiv.org/abs/2504.16925) |
| [321](#ref-321) | Advances in Neural Information Processing Systems , 36:41618‚Äì 41650, 2023. | ‚Äî |
| [322](#ref-322) | Weblinx: Real-world website navigation with multi-turn dialogue | [arXiv:2402.05930](https://arxiv.org/abs/2402.05930)<br/>[Link](https://www.arxiv.org/abs/2402.05930) |
| [323](#ref-323) | Executable code actions elicit better llm agents | [arXiv:2402.01030](https://arxiv.org/abs/2402.01030)<br/>[Link](https://www.arxiv.org/abs/) |
| [324](#ref-324) | REWOO: De- coupling reasoning from observations for efficient augmented language models | [arXiv:2305.18323](https://arxiv.org/abs/2305.18323) |
| [325](#ref-325) | Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration | [arXiv:2311.08562](https://arxiv.org/abs/2311.08562) |
| [338](#ref-338) | Mas-gpt: Training llms to build llm-based multi-agent systems | [arXiv:2503.03686](https://arxiv.org/abs/2503.03686) |
| [347](#ref-347) | Exploratory retrieval-augmented planning for continual embodied instruction following | ‚Äî |
| [348](#ref-348) | Monte carlo tree diffusion for system 2 planning | [arXiv:2502.07202](https://arxiv.org/abs/2502.07202) |
| [353](#ref-353) | Geib, editors, Proceedings of the AAAI 2024 Spring Symposium Series, Stanford, CA, USA, March 25-27, 2024 , pages 595‚Äì597 | [Link](https://doi.org/10.1609/aaaiss.v3i1.31290) |
| [380](#ref-380) | Guardagent: Safeguard llm agents by a guard agent via knowledge-enabled reasoning | [arXiv:2406.09187](https://arxiv.org/abs/2406.09187)<br/>[Link](https://www.arxiv.org/abs/2406.09187) |
| [381](#ref-381) | Magent: A many-agent reinforcement learning platform for artificial collective intelligence | ‚Äî |
| [382](#ref-382) | In Nicoletta Calzolari, Min-Yen Kan, V√©ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International... | [Link](https://aclanthology.org/2024.lrec-main.1413) |
| [383](#ref-383) | Planagent: A multi-modal large language agent for closed-loop vehicle motion planning | [Link](https://api.semanticscholar.org/CorpusID:) |
| [384](#ref-384) | Planagent: A multi-modal large language agent for closed-loop vehicle motion planning | [arXiv:2406.01587](https://arxiv.org/abs/2406.01587) |
| [385](#ref-385) | Deep- researcher: Scaling deep research via reinforcement learning in real-world environments | [arXiv:2504.03160](https://arxiv.org/abs/2504.03160) |
| [386](#ref-386) | Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents | [arXiv:2308.05960](https://arxiv.org/abs/2308.05960)<br/>[Link](https://www.arxiv.org/abs/2308.05960) |
| [387](#ref-387) | Flexplanner: Flexible 3d floorplanning via deep reinforcement learning in hybrid action space with multi-modality representation | ‚Äî |
| [388](#ref-388) | Memorybank: Enhancing large language models with long-term memory | [Link](https://api.semanticscholar.org/) |
| [391](#ref-391) | Multi-agent design: Optimizing agents with better prompts and topologies | [arXiv:2502.02533](https://arxiv.org/abs/2502.02533) |
| [397](#ref-397) | Multiagentbench: Evaluating the collaboration and competition of llm agents | [arXiv:2503.01935](https://arxiv.org/abs/2503.01935) |

[‚Ü• Back to top](#table-of-contents)

---

## Tool Use & Integration

| ID | Paper / Resource | Links |
|---:|---|---|
| [59](#ref-59) | Alphazero-like tree-search can guide large language model decoding and training | [arXiv:2309.17179](https://arxiv.org/abs/2309.17179) |
| [60](#ref-60) | Agentsnet: Coordination and collaborative reasoning in multi-agent llms | [arXiv:2507.08616v1](https://arxiv.org/abs/2507.08616v1) |
| [118](#ref-118) | Nestful: A benchmark for evaluating llms on nested sequences of api calls | [arXiv:2409.03797](https://arxiv.org/abs/2409.03797) |
| [215](#ref-215) | In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 6922‚Äì6939, Singapore, De... | [Link](https://aclanthology.org/2023.findings-emnlp.462/) |
| [243](#ref-243) | Advances in Neural Information Processing Systems , 36:68539‚Äì68551, 2023. | ‚Äî |
| [323](#ref-323) | Executable code actions elicit better llm agents | [arXiv:2402.01030](https://arxiv.org/abs/2402.01030)<br/>[Link](https://www.arxiv.org/abs/) |
| [361](#ref-361) | Proceedings of the Fourth ACM International Conference on AI in Finance , 2023 | [Link](https://api.semanticscholar.org/CorpusID:) |
| [362](#ref-362) | In Proceedings of the 41st International Conference on Machine Learning , 2024. | ‚Äî |
| [363](#ref-363) | Pave your own path: Graph gradual domain adaptation on fused Gromov‚ÄìWasserstein geodesics, 2025. | ‚Äî |
| [364](#ref-364) | Cut the crap: An economical communication pipeline for llm-based multi-agent systems | [arXiv:2410.02506](https://arxiv.org/abs/2410.02506) |

[‚Ü• Back to top](#table-of-contents)

---

## Retrieval-Augmented Generation (RAG)

| ID | Paper / Resource | Links |
|---:|---|---|
| [10](#ref-10) | In NeurIPS 2023 workshop on instruction tuning and instruction following , 2023. | ‚Äî |
| [11](#ref-11) | In The Twelfth International Conference on Learn- ing Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 | ‚Äî |
| [12](#ref-12) | Fieldworkarena: Agentic ai benchmark for real field work tasks | [arXiv:2505.19662](https://arxiv.org/abs/2505.19662)<br/>[Link](https://www.arxiv.org/abs/2505.19662) |
| [13](#ref-13) | Latte: Collaborative test- time adaptation of vision-language models in federated learning | ‚Äî |
| [14](#ref-14) | In Proceedings of the AAAI conference on artificial intelligence , volume 38, pages 17682‚Äì17690, 2024. | ‚Äî |
| [41](#ref-41) | TÀÜ2agent A tool-augmented multimodal misinformation detection agent with monte carlo tree search | [Link](https://doi.org/10.48550/arXiv.2505.19768) |
| [42](#ref-42) | Rezero: Enhancing llm search ability by trying one-more-time | [arXiv:2504.11001](https://arxiv.org/abs/2504.11001) |
| [43](#ref-43) | Trail: Trace reasoning and agentic issue localization | [arXiv:2505.08638](https://arxiv.org/abs/2505.08638) |
| [44](#ref-44) | Balrog: Benchmarking agentic llm and vlm reasoning on games | [arXiv:2411.13543](https://arxiv.org/abs/2411.13543)<br/>[Link](https://www.arxiv.org/abs/2411.13543) |
| [45](#ref-45) | Advances in Neural Information Processing Systems , 36:28091‚Äì28114, 2023. | ‚Äî |
| [46](#ref-46) | Everything of thoughts: Defying the law of penrose triangle for thought generation | [arXiv:2311.04254](https://arxiv.org/abs/2311.04254) |
| [47](#ref-47) | Actionreasoningbench: Reasoning about actions with and without ramification constraints | [arXiv:2406.04046](https://arxiv.org/abs/2406.04046)<br/>[Link](https://www.arxiv.org/abs/2406.04046) |
| [48](#ref-48) | Playing nethack with llms: Potential & limitations as zero-shot agents | [arXiv:2403.00690](https://arxiv.org/abs/2403.00690) |
| [49](#ref-49) | Reinforcement pre-training | [arXiv:2506.08007](https://arxiv.org/abs/2506.08007) |
| [50](#ref-50) | Advances in Neural Information Processing Systems , 37:122556‚Äì122583, 2024. | ‚Äî |
| [54](#ref-54) | From local to global: A graph rag approach to query-focused summarization | [Link](https://api.semanticscholar.org/CorpusID:269363075) |
| [55](#ref-55) | Plan-and-act: Improving planning of agents for long-horizon tasks | [Link](https://doi.org/10.48550/arXiv.2503) |
| [91](#ref-91) | Rag-rl: Advancing retrieval-augmented generation via rl and curriculum learning | [arXiv:2503.12759](https://arxiv.org/abs/2503.12759) |
| [92](#ref-92) | In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual... | [Link](https://doi.org/10.18653/v1/2024.findings-acl.259) |
| [116](#ref-116) | Plan- bench: An extensible benchmark for evaluating large language models on planning and reasoning about change | [arXiv:2206.10498v4](https://arxiv.org/abs/2206.10498v4)<br/>[Link](https://www.arxiv.org/abs/2206.10498v4) |
| [177](#ref-177) | Rag-studio: Towards in-domain adaptation of retrieval augmented generation through self-alignment | ‚Äî |
| [178](#ref-178) | Tree-of-traversals: A zero-shot reasoning algorithm for augmenting black-box language models with knowledge graphs | [arXiv:2407.21358](https://arxiv.org/abs/2407.21358) |
| [191](#ref-191) | Ma-rag: Multi-agent retrieval-augmented generation via collabora- tive chain-of-thought reasoning, 2025 | [Link](https://arxiv.org/abs/2505.20096) |
| [192](#ref-192) | Sfr-rag: Towards contextually faithful llms | [arXiv:2409.09916](https://arxiv.org/abs/2409.09916) |
| [193](#ref-193) | Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation | ‚Äî |
| [258](#ref-258) | Agentic retrieval-augmented generation: A survey on agentic rag | [arXiv:2501.09136](https://arxiv.org/abs/2501.09136) |
| [259](#ref-259) | Real- time anomaly detection and reactive planning with large language models | [arXiv:2407.08735](https://arxiv.org/abs/2407.08735) |
| [327](#ref-327) | Collab-rag: Boosting retrieval-augmented generation for complex question answering via white-box and black-box llm collaboration | [arXiv:2504.04915](https://arxiv.org/abs/2504.04915) |
| [334](#ref-334) | Advances in neural information processing systems , 36:11809‚Äì11822, 2023. | ‚Äî |
| [335](#ref-335) | In International Conference on Learning Representations (ICLR) , 2023. | ‚Äî |
| [336](#ref-336) | In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 | [Link](https://openreview.net/forum?id=WE_vluYUL-X) |
| [337](#ref-337) | Beyond autoregression: Discrete diffusion for complex reasoning and planning | [arXiv:2410.14157](https://arxiv.org/abs/2410.14157) |
| [349](#ref-349) | Rag-kg-il: A multi-agent hybrid framework for reducing hallucinations and enhancing llm reasoning through rag and incremental knowledge graph learning integr... | [Link](https://arxiv.org/abs/2503.13514) |
| [350](#ref-350) | Memagent: Reshaping long-context llm with multi-conv rl-based memory agent | [arXiv:2507.02259](https://arxiv.org/abs/2507.02259) |
| [357](#ref-357) | Agenttuning: Enabling generalized agent abilities for llms | [arXiv:2310.12823](https://arxiv.org/abs/2310.12823) |
| [358](#ref-358) | Glm-4.5: Agentic, reasoning, and coding (arc) foundation models | [arXiv:2508.06471](https://arxiv.org/abs/2508.06471) |
| [371](#ref-371) | Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, 2025 | [Link](https://arxiv.org/abs/2506.12508) |
| [406](#ref-406) | Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms | [arXiv:2506.18896](https://arxiv.org/abs/2506.18896) |
| [407](#ref-407) | Campos Gonzalez Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian Zhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, and A... | [Link](https://doi.org/10) |

[‚Ü• Back to top](#table-of-contents)

---

## Execution

| ID | Paper / Resource | Links |
|---:|---|---|
| [26](#ref-26) | WAPITI: A watermark for finetuned open-source LLMs, 2024. | ‚Äî |
| [27](#ref-27) | Plan-on-graph: Self-correcting adaptive planning of large language model on knowledge graphs | ‚Äî |
| [196](#ref-196) | Reflexion: Language agents with verbal reinforcement learning | [arXiv:2303.11366](https://arxiv.org/abs/2303.11366)<br/>[Link](https://www.arxiv.org/abs/2303.11366) |
| [197](#ref-197) | The hanabi challenge: A new frontier for ai research | [arXiv:1902.00506](https://arxiv.org/abs/1902.00506)<br/>[Link](https://www.arxiv.org/abs/1902.00506) |
| [198](#ref-198) | Comma: A communicative multimodal multi-agent benchmark | [arXiv:2410.07553](https://arxiv.org/abs/2410.07553) |
| [301](#ref-301) | URL https://api.semanticscholar.org/CorpusID:272592995 . | [Link](https://api.semanticscholar.org/CorpusID:272592995) |

[‚Ü• Back to top](#table-of-contents)

---

## Feedback & Reflection

| ID | Paper / Resource | Links |
|---:|---|---|
| [175](#ref-175) | Evaluating very long-term conversational memory of llm agents | [Link](https://api) |
| [188](#ref-188) | In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering , pages 1‚Äì13, 2024. | ‚Äî |
| [189](#ref-189) | Long-horizon planning for multi- agent robots in partially observable environments | ‚Äî |
| [212](#ref-212) | Adapt: As-needed decomposition and planning with language models | [arXiv:2311.05772](https://arxiv.org/abs/2311.05772) |

[‚Ü• Back to top](#table-of-contents)

---

## Memory

| ID | Paper / Resource | Links |
|---:|---|---|
| [1](#ref-1) | [6]Drouin Alexandre, Gasse Maxime, Caccia Massimo, Laradji Issam, H., Verme Manuel, Del, Marty Tom, Boisvert L√©o, Thakkar Megh, Cappart Quentin, Vazquez Davi... | [arXiv:2407.13032](https://arxiv.org/abs/2407.13032)<br/>[arXiv:2310.03903](https://arxiv.org/abs/2310.03903)<br/>[arXiv:2410.08164](https://arxiv.org/abs/2410.08164)<br/>[arXiv:2204.01691](https://arxiv.org/abs/2204.01691)<br/>[arXiv:2312.10003](https://arxiv.org/abs/2312.10003)<br/>[arXiv:2403.07718](https://arxiv.org/abs/2403.07718)<br/>[arXiv:2506.20664v1](https://arxiv.org/abs/2506.20664v1)<br/>[arXiv:2410.20285](https://arxiv.org/abs/2410.20285)<br/>[arXiv:2406.11638](https://arxiv.org/abs/2406.11638)<br/>[Link](https://www.arxiv.org/abs/2403.07718)<br/>[Link](https://www.arxiv.org/abs/2506.20664v1) |
| [38](#ref-38) | The ai scientist: Towards fully automated open-ended scientific discovery | [arXiv:2408.06292v3](https://arxiv.org/abs/2408.06292v3) |
| [39](#ref-39) | Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities | [arXiv:2507.06261](https://arxiv.org/abs/2507.06261) |
| [65](#ref-65) | URL https://www.arxiv.org/abs/2303.17491 . | [arXiv:2303.17491](https://arxiv.org/abs/2303.17491)<br/>[Link](https://www.arxiv.org/abs/2303.17491) |
| [89](#ref-89) | Evaluating memory in llm agents via incremental multi-turn interactions | [arXiv:2507.05257](https://arxiv.org/abs/2507.05257) |
| [90](#ref-90) | Divide and conquer: Grounding llms as efficient decision-making agents via offline hierarchical reinforcement learning | [arXiv:2505.19761](https://arxiv.org/abs/2505.19761) |
| [138](#ref-138) | Preserving diversity in supervised fine-tuning of large language models | [arXiv:2408.16673](https://arxiv.org/abs/2408.16673) |
| [139](#ref-139) | Scm: Enhancing large language model with self-controlled memory framework | [Link](https://api) |
| [140](#ref-140) | Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis | [Link](https://doi.org/10.48550/arXiv.2303.16434) |
| [141](#ref-141) | Visualpredicator: Learning abstract world models with neuro-symbolic predicates for robot planning | [arXiv:2410.23156](https://arxiv.org/abs/2410.23156) |
| [175](#ref-175) | Evaluating very long-term conversational memory of llm agents | [Link](https://api) |
| [176](#ref-176) | Leveraging environment interaction for automated pddl translation and planning with large language models | ‚Äî |
| [185](#ref-185) | Alfworld: Aligning text and embodied environments for interactive learning | [arXiv:2010.03768](https://arxiv.org/abs/2010.03768)<br/>[Link](https://www.arxiv.org/abs/2010.03768) |
| [186](#ref-186) | Welfare diplomacy: Benchmarking language model cooperation | [arXiv:2310.08901](https://arxiv.org/abs/2310.08901) |
| [187](#ref-187) | Webgpt: Browser-assisted question-answering with human feedback | [arXiv:2112.09332](https://arxiv.org/abs/2112.09332) |
| [196](#ref-196) | Reflexion: Language agents with verbal reinforcement learning | [arXiv:2303.11366](https://arxiv.org/abs/2303.11366)<br/>[Link](https://www.arxiv.org/abs/2303.11366) |
| [235](#ref-235) | Pre-act: Multi-step planning and reasoning improves acting in llm agents | [arXiv:2505.09970](https://arxiv.org/abs/2505.09970) |
| [270](#ref-270) | Agent-x: Evaluating deep multimodal reasoning in vision-centric agentic tasks | [arXiv:2505.24876](https://arxiv.org/abs/2505.24876)<br/>[Link](https://www.arxiv.org/abs/2505.24876) |
| [271](#ref-271) | Membench: Towards more comprehensive evaluation on the memory of llm-based agents | [arXiv:2506.21605](https://arxiv.org/abs/2506.21605) |
| [272](#ref-272) | Toolalpaca: Generalized tool learning for language models with 3000 simulated cases | [arXiv:2306.05301](https://arxiv.org/abs/2306.05301) |
| [273](#ref-273) | Toolalpaca: Generalized tool learning for language models with 3000 simulated cases | [Link](https://doi.org/10.48550/arXiv.2306.05301) |
| [274](#ref-274) | In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 | [Link](https://openreview) |
| [275](#ref-275) | Kimi k2: Open agentic intelligence | [arXiv:2507.20534](https://arxiv.org/abs/2507.20534) |
| [276](#ref-276) | 5: Scaling reinforcement learning with llms | [arXiv:2501.12599](https://arxiv.org/abs/2501.12599) |
| [277](#ref-277) | Elitr-bench: A meeting assistant benchmark for long-context language models | [arXiv:2403.20262](https://arxiv.org/abs/2403.20262) |
| [278](#ref-278) | Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments | [arXiv:2404.07972v2](https://arxiv.org/abs/2404.07972v2)<br/>[Link](https://www.arxiv.org/abs/) |
| [279](#ref-279) | R-judge: Benchmarking safety risk awareness for llm agents | [arXiv:2401.10019](https://arxiv.org/abs/2401.10019)<br/>[Link](https://www.arxiv.org/abs/2401.10019) |
| [286](#ref-286) | Storybench: A dynamic benchmark for evaluating long-term memory with multi turns | [arXiv:2506.13356](https://arxiv.org/abs/2506.13356) |
| [287](#ref-287) | Q*: Improving multi-step reasoning for llms with deliberative planning | [arXiv:2406.14283](https://arxiv.org/abs/2406.14283) |
| [299](#ref-299) | Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents | [Link](https://doi.org/10.48550/arXiv.2302.01560) |
| [301](#ref-301) | URL https://api.semanticscholar.org/CorpusID:272592995 . | [Link](https://api.semanticscholar.org/CorpusID:272592995) |
| [302](#ref-302) | URL https://proceedings.neurips.cc/ paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf . | [Link](https://proceedings.neurips.cc/) |
| [303](#ref-303) | Robust watermarking for diffusion models: A unified multi-dimensional recipe, 2024. | ‚Äî |
| [304](#ref-304) | Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution | [arXiv:2502.18449](https://arxiv.org/abs/2502.18449) |
| [305](#ref-305) | Stardojo: Benchmarking open-ended behaviors of agentic multimodal llms in production-living simulations with stardew valley | [arXiv:2507.07445v2](https://arxiv.org/abs/2507.07445v2)<br/>[Link](https://www.arxiv.org/abs/2507) |
| [306](#ref-306) | Codeplan: Unlocking reasoning potential in large language models by scaling code-form planning | ‚Äî |
| [309](#ref-309) | Longmemeval: Benchmarking chat assistants on long-term interactive memory | [arXiv:2410.10813](https://arxiv.org/abs/2410.10813) |
| [310](#ref-310) | Agentic reasoning: Reasoning llms with tools for the deep research | [arXiv:2502.04644](https://arxiv.org/abs/2502.04644) |
| [311](#ref-311) | Chain-of-tools: Utilizing massive unseen tools in the cot reasoning of frozen language models | [Link](https://doi.org/10.48550/arXiv.2503.16779) |
| [312](#ref-312) | URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 2db8ce969b000fe0b3fb172490c33ce8-Paper-Conference.pdf . | [Link](https://proceedings.neurips.cc/paper_files/paper/2024/file/) |
| [313](#ref-313) | Fair anomaly detection for imbalanced groups, 2024. | ‚Äî |
| [314](#ref-314) | URL https://arxiv.org/abs/2502.11705 . | [Link](https://arxiv.org/abs/2502.11705) |
| [315](#ref-315) | Mind2web: Towards a generalist agent for the web | [arXiv:2306.06070](https://arxiv.org/abs/2306.06070) |
| [316](#ref-316) | Medagents: Large language models as collaborators for zero-shot medical reasoning | [arXiv:2311.10537](https://arxiv.org/abs/2311.10537)<br/>[Link](https://www.arxiv.org/abs/2311.10537) |
| [317](#ref-317) | Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning | [arXiv:2503.07459](https://arxiv.org/abs/2503.07459)<br/>[Link](https://www.arxiv.org/abs/2503.07459) |
| [318](#ref-318) | In The Thirteenth International Conference on Learning Representations , 2023. | ‚Äî |
| [328](#ref-328) | A-mem: Agentic memory for llm agents | [arXiv:2502.12110](https://arxiv.org/abs/2502.12110) |
| [329](#ref-329) | In Advances in Neural Information Processing Systems , volume 37, 2024. | ‚Äî |
| [330](#ref-330) | Advances in Neural Information Processing Systems , 37:113519‚Äì113544, 2024. | ‚Äî |
| [331](#ref-331) | In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computa... | [Link](https://aclanthology.org/2025.naacl-long.36/) |
| [332](#ref-332) | A versatile multi-agent reinforcement learning benchmark for inventory management | [arXiv:2306.07542](https://arxiv.org/abs/2306.07542) |
| [354](#ref-354) | CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets | [Link](https://openreview.net/forum?id=G0vdDSt9XM) |
| [355](#ref-355) | Easytool: Enhancing llm-based agents with concise tool instruction | [arXiv:2401.06201](https://arxiv.org/abs/2401.06201) |
| [356](#ref-356) | Infodeepseek: Benchmarking agentic information seeking for retrieval-augmented generation | [arXiv:2505.15872](https://arxiv.org/abs/2505.15872) |
| [376](#ref-376) | In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics, ACL 2025,... | [Link](https://aclanthology.org/2025.findings-acl.811/) |
| [377](#ref-377) | Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents | [arXiv:2507.22844](https://arxiv.org/abs/2507.22844) |
| [378](#ref-378) | Pyvision: Agentic vision with dynamic tooling | [Link](https://doi.org/10.48550/arXiv.2507.07998) |
| [379](#ref-379) | Large language models as commonsense knowledge for large-scale task planning | ‚Äî |
| [389](#ref-389) | Least-to-most prompting enables complex reasoning in large language models | [arXiv:2205.10625](https://arxiv.org/abs/2205.10625) |
| [390](#ref-390) | Dino-wm: World models on pre-trained visual features enable zero-shot planning | [arXiv:2411.04983](https://arxiv.org/abs/2411.04983) |
| [395](#ref-395) | Behaviorgpt: Smart agent simulation for autonomous driving with next-patch prediction | ‚Äî |

[‚Ü• Back to top](#table-of-contents)

---

## Multi-Agent Collaboration

| ID | Paper / Resource | Links |
|---:|---|---|
| [15](#ref-15) | Benchmarl: Benchmarking multi-agent reinforcement learning | ‚Äî |
| [16](#ref-16) | Reflective multi-agent collaboration based on large language models | [Link](https://api.semanticscholar.org/CorpusID:276318441) |
| [17](#ref-17) | Clin: A continually learning language agent for rapid task adaptation and generalization | [arXiv:2310.10134](https://arxiv.org/abs/2310.10134)<br/>[Link](https://www.arxiv.org/abs/2310.10134) |
| [18](#ref-18) | Rt-1: Robotics transformer for real-world control at scale | [arXiv:2212.06817](https://arxiv.org/abs/2212.06817) |
| [19](#ref-19) | O-mapl: Offline multi-agent preference learning | [arXiv:2501.18944](https://arxiv.org/abs/2501.18944) |
| [20](#ref-20) | In The Twelfth International Conference on Learning Representations , 2024 | [Link](https://openreview.net/) |
| [21](#ref-21) | Robot behavior-tree-based task generation with large language models | [arXiv:2302.12927](https://arxiv.org/abs/2302.12927) |
| [22](#ref-22) | In The 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 1788‚Äì1808, 2024. | ‚Äî |
| [23](#ref-23) | In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence , IJCAI ‚Äô24, 2024 | [Link](https://doi.org/10.24963/ijcai.2024/3) |
| [24](#ref-24) | Heterogeneous group-based reinforcement learning for llm-based multi-agent systems | [arXiv:2506.02718](https://arxiv.org/abs/2506.02718) |
| [48](#ref-48) | Playing nethack with llms: Potential & limitations as zero-shot agents | [arXiv:2403.00690](https://arxiv.org/abs/2403.00690) |
| [61](#ref-61) | Magentic-one: A generalist multi-agent system for solving complex tasks | [arXiv:2411.04468](https://arxiv.org/abs/2411.04468) |
| [62](#ref-62) | Stream of search (sos): Learning to search in language | [arXiv:2404.03683](https://arxiv.org/abs/2404.03683) |
| [73](#ref-73) | Mdocagent: A multi-modal multi-agent framework for document understanding [j/ol] | [arXiv:2503.13964](https://arxiv.org/abs/2503.13964) |
| [74](#ref-74) | Autowebglm: A large language model-based web navigating agent | [arXiv:2404.03648](https://arxiv.org/abs/2404.03648)<br/>[Link](https://www.arxiv.org/abs/2404.03648) |
| [75](#ref-75) | Reasoning with language model is planning with world model | [arXiv:2305.14992](https://arxiv.org/abs/2305.14992) |
| [86](#ref-86) | Webvoyager: Building an end-to-end web agent with large multimodal models | [arXiv:2401.13919](https://arxiv.org/abs/2401.13919)<br/>[Link](https://www.arxiv.org/abs/2401.13919) |
| [87](#ref-87) | Tool documentation enables zero-shot tool-usage with large language models, 2023 | [Link](https://arxiv.org/abs/2308.00675) |
| [88](#ref-88) | Learning multi-agent communication from graph modeling perspective | [arXiv:2405.08550](https://arxiv.org/abs/2405.08550) |
| [100](#ref-100) | Reflect-then-plan: Offline model-based planning through a doubly bayesian lens | [arXiv:2506.06261](https://arxiv.org/abs/2506.06261) |
| [101](#ref-101) | Enhancing multi-agent systems via reinforcement learning with llm-based planner and graph-based policy | [arXiv:2503.10049](https://arxiv.org/abs/2503.10049) |
| [102](#ref-102) | Webwalker: Benchmarking llms in web traversal | [arXiv:2501.07572v2](https://arxiv.org/abs/2501.07572v2)<br/>[Link](https://www.arxiv.org/abs/2501.07572v2) |
| [103](#ref-103) | ACM Transactions on Software Engineering and Methodology , 33(7):1‚Äì30, 2024. | ‚Äî |
| [109](#ref-109) | Crew-wildfire: Benchmarking agentic multi-agent collaborations at scale | [arXiv:2507.05178v1](https://arxiv.org/abs/2507.05178v1)<br/>[Link](https://www.arxiv.org/abs/2507) |
| [110](#ref-110) | Avalonbench: Evaluating llms playing the game of avalon | [arXiv:2310.05036](https://arxiv.org/abs/2310.05036)<br/>[Link](https://www.arxiv.org/abs/2310.05036) |
| [111](#ref-111) | Agentic reasoning and tool integration for llms via reinforcement learning | [arXiv:2505.01441v1](https://arxiv.org/abs/2505.01441v1)<br/>[Link](https://www.arxiv.org/) |
| [112](#ref-112) | Agentic reasoning: Reasoning llms with tools for the deep research | [arXiv:2502.04644](https://arxiv.org/abs/2502.04644)<br/>[Link](https://www.arxiv.org/abs/2502.04644) |
| [113](#ref-113) | Laser: Llm agent with state-space exploration for web navigation | [arXiv:2309.08172](https://arxiv.org/abs/2309.08172)<br/>[Link](https://www) |
| [114](#ref-114) | Retrointext: A multimodal large language model enhanced framework for retrosynthetic planning via in-context representation learning | ‚Äî |
| [125](#ref-125) | Imp-marl: a suite of environments for large-scale infrastructure management planning via marl | ‚Äî |
| [126](#ref-126) | Intellagent: A multi-agent framework for evaluating conversational ai systems | [arXiv:2501.11067](https://arxiv.org/abs/2501.11067) |
| [127](#ref-127) | In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Pro- cessing System... | [Link](https://proceedings.neurips.cc/paper/2020/hash/) |
| [128](#ref-128) | Advances in neural information processing systems , 33:9459‚Äì9474, 2020. | ‚Äî |
| [129](#ref-129) | Agent-oriented planning in multi-agent systems | [arXiv:2410.02189](https://arxiv.org/abs/2410.02189) |
| [130](#ref-130) | Adaptive graph pruning for multi-agent communi- cation | [arXiv:2506.02951](https://arxiv.org/abs/2506.02951) |
| [131](#ref-131) | Smoa: Improving multi-agent large language models with sparse mixture-of-agents, 2024 | [Link](https://arxiv.org/abs/2411) |
| [132](#ref-132) | In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Si... | [Link](https://doi.org/10.18653/v1/) |
| [133](#ref-133) | Model-free graph data selection under distribution shift, 2025. | ‚Äî |
| [134](#ref-134) | Search-o1: Agentic search-enhanced large reasoning models | [arXiv:2501.05366](https://arxiv.org/abs/2501.05366) |
| [135](#ref-135) | Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent | [arXiv:2411.02937](https://arxiv.org/abs/2411.02937) |
| [152](#ref-152) | Llm collaboration with multi-agent reinforcement learning | [arXiv:2508.04652](https://arxiv.org/abs/2508.04652) |
| [153](#ref-153) | Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts | [arXiv:2310.14628](https://arxiv.org/abs/2310.14628) |
| [154](#ref-154) | Tool-planner: Task planning with clusters across multiple tools | [arXiv:2406.03807](https://arxiv.org/abs/2406.03807) |
| [164](#ref-164) | Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence | [arXiv:2503.10265](https://arxiv.org/abs/2503.10265) |
| [165](#ref-165) | Chameleon: Plug-and-play compositional reasoning with large language models | ‚Äî |
| [166](#ref-166) | Octotools: An agentic framework with extensible tools for complex reasoning, 2025 | [Link](https://arxiv.org/abs/2502.11271) |
| [167](#ref-167) | In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Vo... | [Link](https://aclanthology.org/2024.eacl-long.7/) |
| [168](#ref-168) | Wilbur: Adaptive in-context learning for robust and accurate web agents | [arXiv:2404.05902](https://arxiv.org/abs/2404.05902) |
| [173](#ref-173) | Advancing tool- augmented large language models via meta-verification and reflection learning | [Link](https://doi.org/10.48550/arXiv.2506.04625) |
| [174](#ref-174) | Advances in Neural Information Processing Systems , 36:46534‚Äì46594, 2023. | ‚Äî |
| [199](#ref-199) | Repograph: Enhancing ai software engineering with repository-level code graph | [Link](https://api.semanticscholar.org/CorpusID:273502041) |
| [200](#ref-200) | In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models , volume 2, 2024. | ‚Äî |
| [201](#ref-201) | Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez | [Link](https://api.semanticscholar) |
| [202](#ref-202) | Training software engineering agents and verifiers with swe-gym | [arXiv:2412.21139](https://arxiv.org/abs/2412.21139) |
| [203](#ref-203) | Art: Automatic multi-step reasoning and tool-use for large language models, 2023 | ‚Äî |
| [204](#ref-204) | Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning | [arXiv:2502.18439](https://arxiv.org/abs/2502.18439) |
| [205](#ref-205) | Large language models can self-improve at web agent tasks | [Link](https://api.semanticscholar.org/CorpusID:270122967) |
| [206](#ref-206) | Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, Neu... | [Link](http://papers.nips.cc/paper_files/paper/2024/hash/) |
| [207](#ref-207) | Advances in Neural Information Processing Systems , 37:126544‚Äì126565, 2024. | ‚Äî |
| [208](#ref-208) | Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese | [arXiv:2504.19314](https://arxiv.org/abs/2504.19314)<br/>[Link](https://www.arxiv.org/abs/2504.19314) |
| [209](#ref-209) | Quantifying the reasoning abilities of llms on real-world clinical cases | [arXiv:2503.04691](https://arxiv.org/abs/2503.04691)<br/>[Link](https://www.arxiv.org/abs/2503.04691) |
| [210](#ref-210) | Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents | [arXiv:2406.06769](https://arxiv.org/abs/2406.06769)<br/>[Link](https://www.arxiv) |
| [211](#ref-211) | Are retrials all you need? enhancing large language model reasoning without verbalized feedback | [arXiv:2504.12951](https://arxiv.org/abs/2504.12951) |
| [232](#ref-232) | URL https://www.arxiv.org/abs/2309.09971 . | [arXiv:2309.09971](https://arxiv.org/abs/2309.09971)<br/>[Link](https://www.arxiv.org/abs/2309.09971) |
| [233](#ref-233) | In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA , volume 229 of Procee... | [Link](https://proceedings.mlr.press/v229/rana23a.html) |
| [237](#ref-237) | Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents | [arXiv:2406.14884](https://arxiv.org/abs/2406.14884)<br/>[Link](https://www.arxiv.org/abs/2406.14884) |
| [243](#ref-243) | Advances in Neural Information Processing Systems , 36:68539‚Äì68551, 2023. | ‚Äî |
| [244](#ref-244) | Mastering board games by external and internal planning with language models | [arXiv:2412.12119](https://arxiv.org/abs/2412.12119) |
| [262](#ref-262) | In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023 , pages 2986‚Äì2997 | [Link](https://doi.org/10.1109/ICCV51070.2023.00280) |
| [265](#ref-265) | Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design | [arXiv:2310.14420](https://arxiv.org/abs/2310.14420) |
| [266](#ref-266) | Zerosearch: Incentivize the search capability of llms without searching | [arXiv:2505.04588](https://arxiv.org/abs/2505.04588) |
| [267](#ref-267) | In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EA... | [Link](https://aclanthology.org/2024.eacl-long.29) |
| [268](#ref-268) | In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1251‚Äì1261, 2025. | ‚Äî |
| [269](#ref-269) | Dynamic cheatsheet: Test-time learning with adaptive memory | [arXiv:2504.07952](https://arxiv.org/abs/2504.07952) |
| [280](#ref-280) | Automl-agent: A multi-agent llm framework for full-pipeline automl | [arXiv:2410.02958](https://arxiv.org/abs/2410.02958) |
| [281](#ref-281) | Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions | [arXiv:2212.10509](https://arxiv.org/abs/2212.10509) |
| [282](#ref-282) | In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol... | [Link](https://aclanthology.org/2023.acl-long) |
| [285](#ref-285) | From an llm swarm to a pddl-empowered hive: Planning self-executed instructions in a multi-modal jungle | [arXiv:2412.12839](https://arxiv.org/abs/2412.12839) |
| [296](#ref-296) | In Forty-first International Conference on Machine Learning , 2024. | ‚Äî |
| [297](#ref-297) | InForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 | [Link](https://openreview.net/forum?id=p0lKWzdikQ) |
| [298](#ref-298) | URL https://api.semanticscholar.org/CorpusID:276094378 . | [Link](https://api.semanticscholar.org/CorpusID:276094378) |
| [300](#ref-300) | M3hf: Multi-agent reinforcement learning from multi- phase human feedback of mixed quality | [arXiv:2503.02077](https://arxiv.org/abs/2503.02077) |
| [326](#ref-326) | On the tool manipulation capability of open-source large language models | [Link](https://doi.org/10.48550/arXiv.2305.16504) |
| [333](#ref-333) | Advances in Neural Information Processing Systems , 37:10470‚Äì10490, 2024. | ‚Äî |
| [339](#ref-339) | Rational decision-making agent with internalized utility judgment | [arXiv:2308.12519](https://arxiv.org/abs/2308.12519) |
| [340](#ref-340) | Webcanvas: Benchmarking web agents in online environments | [arXiv:2406.12373v3](https://arxiv.org/abs/2406.12373v3)<br/>[Link](https://www.arxiv.org/abs/2406.12373v3) |
| [341](#ref-341) | Restgpt: Connecting large language models with real-world restful apis | [arXiv:2306.06624](https://arxiv.org/abs/2306.06624)<br/>[Link](https://www.arxiv.org/abs/2306.06624) |
| [342](#ref-342) | Agent lumos: Unified and modular training for open-source language agents | [arXiv:2311.05657](https://arxiv.org/abs/2311.05657) |
| [343](#ref-343) | Medagentbench: A realistic virtual ehr environment to benchmark medical llm agents | [arXiv:2501.14654](https://arxiv.org/abs/2501.14654)<br/>[Link](https://www.arxiv.org/abs/2501.14654) |
| [344](#ref-344) | In Proceedings of the ACM on Web Conference 2024 , pages 3667‚Äì3678, 2024. | ‚Äî |
| [345](#ref-345) | In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2025. | ‚Äî |
| [346](#ref-346) | In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 2025. | ‚Äî |
| [351](#ref-351) | Mineland: Simulating large-scale multi-agent interactions with limited multimodal senses and physical needs | [arXiv:2403.19267](https://arxiv.org/abs/2403.19267) |
| [352](#ref-352) | Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning | [arXiv:2305.13660](https://arxiv.org/abs/2305.13660) |
| [359](#ref-359) | InForty-second International Conference on Machine Learning . | ‚Äî |
| [360](#ref-360) | Boosting tool use of large language models via iterative reinforced fine-tuning | ‚Äî |
| [365](#ref-365) | G-designer: Architecting multi-agent communication topologies via graph neural networks | [arXiv:2410.11782](https://arxiv.org/abs/2410.11782) |
| [366](#ref-366) | Multi-agent architecture search via agentic supernet | [arXiv:2502.04180](https://arxiv.org/abs/2502.04180) |
| [367](#ref-367) | Learning to retrieve and reason on knowledge graph through active self-reflection | [arXiv:2502.14932](https://arxiv.org/abs/2502.14932) |
| [368](#ref-368) | Aflow: Automating agentic workflow generation | [arXiv:2410.10762](https://arxiv.org/abs/2410.10762) |
| [369](#ref-369) | In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 | [Link](https://openreview.net/forum?id=) |
| [370](#ref-370) | Raft: Adapting language model to domain specific rag | [arXiv:2403.10131](https://arxiv.org/abs/2403.10131) |
| [372](#ref-372) | Metaagent: Automatically constructing multi-agent systems based on finite state machines | [arXiv:2507.22606](https://arxiv.org/abs/2507.22606) |
| [373](#ref-373) | Turnbench- ms: A benchmark for evaluating multi-turn, multi-step reasoning in large language models | [arXiv:2506.01341](https://arxiv.org/abs/2506.01341) |
| [374](#ref-374) | In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024 | [Link](https://openreview.net/forum?id=LuCLf4BJsr) |
| [375](#ref-375) | Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants | [arXiv:2409.20163](https://arxiv.org/abs/2409.20163) |
| [382](#ref-382) | In Nicoletta Calzolari, Min-Yen Kan, V√©ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International... | [Link](https://aclanthology.org/2024.lrec-main.1413) |
| [392](#ref-392) | Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving | [arXiv:2010.09776](https://arxiv.org/abs/2010.09776) |
| [393](#ref-393) | Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks | [arXiv:2503.15478](https://arxiv.org/abs/2503.15478) |
| [394](#ref-394) | MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents | [arXiv:2506.15841](https://arxiv.org/abs/2506.15841) |
| [396](#ref-396) | Lamarl: Llm-aided multi-agent reinforcement learning for cooperative policy generation | ‚Äî |
| [397](#ref-397) | Multiagentbench: Evaluating the collaboration and competition of llm agents | [arXiv:2503.01935](https://arxiv.org/abs/2503.01935) |
| [398](#ref-398) | Inters: Unlocking the power of large language models in search with instruction tuning | [arXiv:2401.06532](https://arxiv.org/abs/2401.06532) |
| [399](#ref-399) | In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: An... | [Link](http://papers.nips.cc/paper_files/paper/2023/hash/) |
| [400](#ref-400) | In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 | [Link](https://openreview.net/forum?id=B6pQxqUcT8) |
| [401](#ref-401) | Of- ficebench: Benchmarking language agents across multiple applications for office automation | [arXiv:2407.19056](https://arxiv.org/abs/2407.19056)<br/>[Link](https://www.arxiv.org/abs/2407.19056) |
| [407](#ref-407) | Campos Gonzalez Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian Zhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, and A... | [Link](https://doi.org/10) |

[‚Ü• Back to top](#table-of-contents)

---

## Applications & Benchmarks

| ID | Paper / Resource | Links |
|---:|---|---|
| [125](#ref-125) | Imp-marl: a suite of environments for large-scale infrastructure management planning via marl | ‚Äî |
| [209](#ref-209) | Quantifying the reasoning abilities of llms on real-world clinical cases | [arXiv:2503.04691](https://arxiv.org/abs/2503.04691)<br/>[Link](https://www.arxiv.org/abs/2503.04691) |
| [279](#ref-279) | R-judge: Benchmarking safety risk awareness for llm agents | [arXiv:2401.10019](https://arxiv.org/abs/2401.10019)<br/>[Link](https://www.arxiv.org/abs/2401.10019) |
| [309](#ref-309) | Longmemeval: Benchmarking chat assistants on long-term interactive memory | [arXiv:2410.10813](https://arxiv.org/abs/2410.10813) |
| [386](#ref-386) | Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents | [arXiv:2308.05960](https://arxiv.org/abs/2308.05960)<br/>[Link](https://www.arxiv.org/abs/2308.05960) |
| [401](#ref-401) | Of- ficebench: Benchmarking language agents across multiple applications for office automation | [arXiv:2407.19056](https://arxiv.org/abs/2407.19056)<br/>[Link](https://www.arxiv.org/abs/2407.19056) |
| [402](#ref-402) | Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery | [arXiv:2410.05080](https://arxiv.org/abs/2410.05080) |
| [403](#ref-403) | Rt-2: Vision-language-action models transfer web knowledge to robotic control | ‚Äî |
| [404](#ref-404) | Transformer copilot: Learning from the mistake log in LLM fine-tuning, 2025. | ‚Äî |
| [405](#ref-405) | Gtr: Graph-table- rag for cross-table question answering | [arXiv:2504.01346](https://arxiv.org/abs/2504.01346) |

[‚Ü• Back to top](#table-of-contents)

---

## Full References
<details><summary><strong>Show / Hide All</strong></summary>

<a id='ref-1'></a>
### [1]
Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku. Agent- e: From autonomous web navigation to foundational design principles in agentic systems. arXiv preprint arXiv:2407.13032 , 2024. [2]Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. Llm-coordination: evaluating and analyzing multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903 , 2023. [3]Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164 , 2024. [4]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 , 2022. [5]Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003 , 2023. [6]Drouin Alexandre, Gasse Maxime, Caccia Massimo, Laradji Issam, H., Verme Manuel, Del, Marty Tom, Boisvert L√©o, Thakkar Megh, Cappart Quentin, Vazquez David, Chapados Nicolas, and Lacoste Alexandre. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718 , 2024. URL https://www.arxiv.org/abs/2403.07718 . [7]Lupu Andrei, Willi Timon, and Foerster Jakob. The decrypto benchmark for multi-agent reasoning and theory of mind. arXiv preprint arXiv:2506.20664v1 , 2025. URL https://www.arxiv.org/abs/2506.20664v1 . [8]Antonis Antoniades, Albert √ñrwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. Swe-search: En- hancing software agents with monte carlo tree search and iterative refinement. arXiv preprint arXiv:2410.20285 , 2024. [9]Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for software-engineering ai agents. arXiv preprint arXiv:2406.11638 , 2024.

<a id='ref-10'></a>
### [10]
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Self-reflective retrieval augmented generation. In NeurIPS 2023 workshop on instruction tuning and instruction following , 2023.

<a id='ref-11'></a>
### [11]
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to re- trieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learn- ing Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https: //openreview.net/forum?id=hSyW5go0v8 .

<a id='ref-12'></a>
### [12]
Moteki Atsunori, Masui Shoichi, Yang Fan, Song Yueqi, Bisk Yonatan, Neubig Graham, Kusajima Ikuo, Watanabe Yasuto, Ishida Hiroyuki, Takahashi Jun, and Jiang Shan. Fieldworkarena: Agentic ai benchmark for real field work tasks. arXiv preprint arXiv:2505.19662 , 2025. URL https://www.arxiv.org/abs/2505.19662 .

<a id='ref-13'></a>
### [13]
Wenxuan Bao, Ruxi Deng, Ruizhong Qiu, Tianxin Wei, Hanghang Tong, and Jingrui He. Latte: Collaborative test- time adaptation of vision-language models in federated learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2025.

<a id='ref-14'></a>
### [14]
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence , volume 38, pages 17682‚Äì17690, 2024.

<a id='ref-15'></a>
### [15]
Matteo Bettini, Amanda Prorok, and Vincent Moens. Benchmarl: Benchmarking multi-agent reinforcement learning. Journal of Machine Learning Research , 25(217):1‚Äì10, 2024.

<a id='ref-16'></a>
### [16]
Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. Reflective multi-agent collaboration based on large language models. In Neural Information Processing Systems , 2024. URL https://api.semanticscholar.org/CorpusID:276318441 .

<a id='ref-17'></a>
### [17]
Majumder Bodhisattwa, Prasad, Mishra Bhavana, Dalvi, Jansen Peter, Tafjord Oyvind, Tandon Niket, Zhang Li, Callison-Burch Chris, and Clark Peter. Clin: A continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134 , 2023. URL https://www.arxiv.org/abs/2310.10134 . 20 APREPRINT -

<a id='ref-18'></a>
### [18]
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817 , 2022.

<a id='ref-19'></a>
### [19]
The Viet Bui, Tien Mai, and Hong Thanh Nguyen. O-mapl: Offline multi-agent preference learning. arXiv preprint arXiv:2501.18944 , 2025.

<a id='ref-20'></a>
### [20]
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/ forum?id=qV83K9d5WB .

<a id='ref-21'></a>
### [21]
Yue Cao and CS Lee. Robot behavior-tree-based task generation with large language models. arXiv preprint arXiv:2302.12927 , 2023.

<a id='ref-22'></a>
### [22]
Eunice Chan, Zhining Liu, Ruizhong Qiu, Yuheng Zhang, Ross Maciejewski, and Hanghang Tong. Group fairness via group consensus. In The 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 1788‚Äì1808, 2024.

<a id='ref-23'></a>
### [23]
Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B√∂rje Karlsson, Jie Fu, and Yemin Shi. Autoagents: a framework for automatic agent generation. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence , IJCAI ‚Äô24, 2024. ISBN 978-1-956792-04-1. doi: 10.24963/ijcai.2024/3. URL https://doi.org/10.24963/ijcai.2024/3 .

<a id='ref-24'></a>
### [24]
Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, and Zenglin Xu. Heterogeneous group-based reinforcement learning for llm-based multi-agent systems. arXiv preprint arXiv:2506.02718 , 2025.

<a id='ref-25'></a>
### [25]
Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, and Qingyao Ai. Enhancing llm-based agents via global planning and hierarchical execution. arXiv preprint arXiv:2504.16563 , 2025.

<a id='ref-26'></a>
### [26]
Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, Tianxin Wei, Hyunsik Yoo, Zhichen Zeng, Deqing Yang, and Hanghang Tong. WAPITI: A watermark for finetuned open-source LLMs, 2024.

<a id='ref-27'></a>
### [27]
Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong. Plan-on-graph: Self-correcting adaptive planning of large language model on knowledge graphs. Advances in Neural Information Processing Systems , 37:37665‚Äì37691, 2024.

<a id='ref-28'></a>
### [28]
Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.19470 .

<a id='ref-29'></a>
### [29]
Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470 , 2025.

<a id='ref-30'></a>
### [30]
Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng Zhao. T-eval: Evaluating the tool utilization capability of large language models step by step. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 , pages 9510‚Äì9529. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.515. URL https://doi.org/10.18653/v1/2024.acl-long.515 .

<a id='ref-31'></a>
### [31]
Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. Scaling autonomous agents via automatic reward modeling and planning. arXiv preprint arXiv:2502.12130 , 2025.

<a id='ref-32'></a>
### [32]
Zhiliang Chen, Xinyuan Niu, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. Broaden your scope! efficient multi-turn conversation planning for llms with semantic space. arXiv preprint arXiv:2503.11586 , 2025.

<a id='ref-33'></a>
### [33]
Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, and Ji-Rong Wen. ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 14777‚Äì14790, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 985. URL https://aclanthology.org/2023.findings-emnlp.985/ .

<a id='ref-34'></a>
### [34]
Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890 , 2024.

<a id='ref-35'></a>
### [35]
Zixuan Chen, Jing Huo, Yangtao Chen, and Yang Gao. Robohorizon: An llm-assisted multi-view world model for long-horizon robotic manipulation. arXiv preprint arXiv:2501.06605 , 2025.

<a id='ref-36'></a>
### [36]
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935 , 2024. 21 APREPRINT -

<a id='ref-37'></a>
### [37]
Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production- ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413 , 2025.

<a id='ref-38'></a>
### [38]
Lu Chris, Lu Cong, Lange Robert, Tjarko, Foerster Jakob, Clune Jeff, and Ha David. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292v3 , 2024. URL https: //www.arxiv.org/abs/2408.06292v3 .

<a id='ref-39'></a>
### [39]
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025.

<a id='ref-40'></a>
### [40]
Cristina Cornelio, Flavio Petruzzellis, and Pietro Lio. Hierarchical planning for complex tasks with knowledge graph-rag and symbolic verification. arXiv preprint arXiv:2504.04578 , 2025.

<a id='ref-41'></a>
### [41]
Xing Cui, Yueying Zou, Zekun Li, Pei-Pei Li, Xinyuan Xu, Xuannan Liu, Huaibo Huang, and Ran He. TÀÜ2agent A tool-augmented multimodal misinformation detection agent with monte carlo tree search. CoRR , abs/2505.19768, 2025. doi: 10.48550/ARXIV .2505.19768. URL https://doi.org/10.48550/arXiv.2505.19768 .

<a id='ref-42'></a>
### [42]
Alan Dao and Thinh Le. Rezero: Enhancing llm search ability by trying one-more-time. arXiv preprint arXiv:2504.11001 , 2025.

<a id='ref-43'></a>
### [43]
Deshpande Darshan, Gangal Varun, Mehta Hersh, Krishnan Jitin, Kannappan Anand, and Qian Rebecca. Trail: Trace reasoning and agentic issue localization. arXiv preprint arXiv:2505.08638 , 2025. URL https: //www.arxiv.org/abs/2505.08638 .

<a id='ref-44'></a>
### [44]
Paglieri Davide, Cupia≈Ç Bart≈Çomiej, Coward Samuel, Piterbarg Ulyana, Wolczyk Maciej, Khan Akbir, Pignatelli Eduardo, Kuci ¬¥nski ≈Åukasz, Pinto Lerrel, Fergus Rob, Foerster Jakob, Nicolaus, Parker-Holder Jack, and Rock- t√§schel Tim. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543 , 2024. URL https://www.arxiv.org/abs/2411.13543 .

<a id='ref-45'></a>
### [45]
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems , 36:28091‚Äì28114, 2023.

<a id='ref-46'></a>
### [46]
Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254 , 2023.

<a id='ref-47'></a>
### [47]
Handa Divij, Dolin Pavel, Kumbhar Shrinidhi, Son Tran, Cao, and Baral Chitta. Actionreasoningbench: Reasoning about actions with and without ramification constraints. arXiv preprint arXiv:2406.04046 , 2024. URL https://www.arxiv.org/abs/2406.04046 .

<a id='ref-48'></a>
### [48]
Jeurissen Dominik, Perez-Liebana Diego, Gow Jeremy, Cakmak Duygu, and Kwan James. Playing nethack with llms: Potential & limitations as zero-shot agents. arXiv preprint arXiv:2403.00690 , 2024. URL https: //www.arxiv.org/abs/2403.00690 .

<a id='ref-49'></a>
### [49]
Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement pre-training. arXiv preprint arXiv:2506.08007 , 2025.

<a id='ref-50'></a>
### [50]
Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li, and Yan Zheng. Diffuserlite: Towards real-time diffusion planning. Advances in Neural Information Processing Systems , 37:122556‚Äì122583, 2024.

<a id='ref-51'></a>
### [51]
Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Re-rest: Reflection-reinforced self-training for language agents. arXiv preprint arXiv:2406.01495 , 2024.

<a id='ref-52'></a>
### [52]
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023.

<a id='ref-53'></a>
### [53]
Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. Perltqa: A personal long-term memory dataset for memory classification, retrieval, and fusion in question answering. In Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10) , pages 152‚Äì164, 2024.

<a id='ref-54'></a>
### [54]
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. ArXiv , abs/2404.16130, 2024. URL https://api.semanticscholar.org/CorpusID:269363075 .

<a id='ref-55'></a>
### [55]
Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks. CoRR , abs/2503.09572, 2025. doi: 10.48550/ARXIV .2503.09572. URL https://doi.org/10.48550/arXiv.2503. 09572 . 22 APREPRINT -

<a id='ref-56'></a>
### [56]
Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks. arXiv preprint arXiv:2503.09572 , 2025.

<a id='ref-57'></a>
### [57]
Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint arXiv:2504.21024 , 2025.

<a id='ref-58'></a>
### [58]
Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536 , 2025.

<a id='ref-59'></a>
### [59]
Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179 , 2023.

<a id='ref-60'></a>
### [60]
Gr√∂tschla Florian, M√ºller Luis, T√∂nshoff Jan, Galkin Mikhail, and Perozzi Bryan. Agentsnet: Coordination and collaborative reasoning in multi-agent llms. arXiv preprint arXiv:2507.08616v1 , 2025. URL https: //www.arxiv.org/abs/2507.08616v1 .

<a id='ref-61'></a>
### [61]
Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, et al. Magentic-one: A generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468 , 2024.

<a id='ref-62'></a>
### [62]
Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683 , 2024.

<a id='ref-63'></a>
### [63]
Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning as general-purpose manipulation world model. arXiv preprint arXiv:2412.08261 , 2024.

<a id='ref-64'></a>
### [64]
Longling Geng and Edward Y Chang. Realm-bench: A real-world planning benchmark for llms and multi-agent systems. arXiv preprint arXiv:2502.18836 , 2025.

<a id='ref-65'></a>
### [65]
Kim Geunwoo, Baldi Pierre, and McAleer Stephen. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491 , 2023. URL https://www.arxiv.org/abs/2303.17491 .

<a id='ref-66'></a>
### [66]
Olga Golovneva, Sean O‚ÄôBrien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel- Zarandi, and Asli Celikyilmaz. Pathfinder: Guided search over multi-step reasoning paths. arXiv preprint arXiv:2312.05180 , 2023.

<a id='ref-67'></a>
### [67]
Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559 , 2024.

<a id='ref-68'></a>
### [68]
Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems , 36:79081‚Äì79094, 2023.

<a id='ref-69'></a>
### [69]
Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models. arXiv preprint arXiv:2502.01142 , 2025.

<a id='ref-70'></a>
### [70]
Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jinjie Gu, and Chenyi Zhuang. Intelligent virtual assistants with llm-based process automation. arXiv preprint arXiv:2312.06677 , 2023.

<a id='ref-71'></a>
### [71]
Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, and Feng Wu. Hypertree planning: Enhancing llm reasoning via hierarchical thinking. arXiv preprint arXiv:2505.02322 , 2025.

<a id='ref-72'></a>
### [72]
Tanmay Gupta, Luca Weihs, and Aniruddha Kembhavi. Codenav: Beyond tool-use to using real-world codebases with llm agents. arXiv preprint arXiv:2406.12276 , 2024.

<a id='ref-73'></a>
### [73]
SW HAN, P XIA, RY ZHANG, et al. Mdocagent: A multi-modal multi-agent framework for document understanding [j/ol]. arXiv preprint arXiv:2503.13964 , 2025.

<a id='ref-74'></a>
### [74]
Lai Hanyu, Liu Xiao, Iong Iat, Long, Yao Shuntian, Chen Yuxuan, Shen Pengbo, Yu Hao, Zhang Hanchen, Zhang Xiaohan, Dong Yuxiao, and Tang Jie. Autowebglm: A large language model-based web navigating agent. arXiv preprint arXiv:2404.03648 , 2024. URL https://www.arxiv.org/abs/2404.03648 .

<a id='ref-75'></a>
### [75]
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992 , 2023. 23 APREPRINT -

<a id='ref-76'></a>
### [76]
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language mod- els with massive tools via tool embeddings. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html .

<a id='ref-77'></a>
### [77]
Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. LLM reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 , 2024.

<a id='ref-78'></a>
### [78]
Yilun Hao, Yang Zhang, and Chuchu Fan. Planning anything with rigor: General-purpose zero-shot planning with llm-based formalized programming. arXiv preprint arXiv:2410.12112 , 2024.

<a id='ref-79'></a>
### [79]
Kokel Harsha, Katz Michael, Srinivas Kavitha, and Sohrabi Shirin. Acpbench: Reasoning about action, change, and planning. arXiv preprint arXiv:2410.05669 , 2024. URL https://www.arxiv.org/abs/2410.05669 .

<a id='ref-80'></a>
### [80]
Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, and Huimin Ma. Enhancing llm reasoning with multi-path collaborative reactive and reflection agents. arXiv preprint arXiv:2501.00430 , 2024.

<a id='ref-81'></a>
### [81]
Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919 , 2024.

<a id='ref-82'></a>
### [82]
Xinyu He, Jian Kang, Ruizhong Qiu, Fei Wang, Jose Sepulveda, and Hanghang Tong. On the sensitivity of individual fairness: Measures and robust algorithms. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management , pages 829‚Äì838, 2024.

<a id='ref-83'></a>
### [83]
Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553 , 2024.

<a id='ref-84'></a>
### [84]
Hu Hengyuan, Lerer Adam, Cui Brandon, Wu David, Pineda Luis, Brown Noam, and Foerster Jakob. Off-belief learning. arXiv preprint arXiv:2103.04000 , 2021. URL https://www.arxiv.org/abs/2103.04000 .

<a id='ref-85'></a>
### [85]
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J√ºrgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= VtmBAGCN7o .

<a id='ref-86'></a>
### [86]
He Hongliang, Yao Wenlin, Ma Kaixin, Yu Wenhao, Dai Yong, Zhang Hongming, Lan Zhenzhong, and Yu Dong. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919 , 2024. URL https://www.arxiv.org/abs/2401.13919 .

<a id='ref-87'></a>
### [87]
Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models, 2023. URL https://arxiv.org/abs/2308.00675 .

<a id='ref-88'></a>
### [88]
Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Learning multi-agent communication from graph modeling perspective. arXiv preprint arXiv:2405.08550 , 2024.

<a id='ref-89'></a>
### [89]
Yuanzhe Hu, Yu Wang, and Julian McAuley. Evaluating memory in llm agents via incremental multi-turn interactions. arXiv preprint arXiv:2507.05257 , 2025.

<a id='ref-90'></a>
### [90]
Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, and Yu Cheng. Divide and conquer: Grounding llms as efficient decision-making agents via offline hierarchical reinforcement learning. arXiv preprint arXiv:2505.19761 , 2025.

<a id='ref-91'></a>
### [91]
Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Hao Peng, Julia Hockenmaier, and Tong Zhang. Rag-rl: Advancing retrieval-augmented generation via rl and curriculum learning. arXiv preprint arXiv:2503.12759 , 2025.

<a id='ref-92'></a>
### [92]
Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024 , pages 4363‚Äì4400. Association for Computational Linguistics, 2024. doi: 10. 18653/V1/2024.FINDINGS-ACL.259. URL https://doi.org/10.18653/v1/2024.findings-acl.259 . 24 APREPRINT -

<a id='ref-93'></a>
### [93]
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608 , 2022.

<a id='ref-94'></a>
### [94]
Yizheng Huang and Jimmy Huang. A survey on retrieval-augmented text generation for large language models. arXiv preprint arXiv:2404.10981 , 2024.

<a id='ref-95'></a>
### [95]
Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models: Deciding whether to use tools and which to use. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=R0c2qtalgG .

<a id='ref-96'></a>
### [96]
Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao Kurohashi. MultiTool-CoT: GPT-3 can use multiple external tools with chain of thought prompting. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1522‚Äì1532, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.130. URL https://aclanthology.org/2023.acl-short.130/ .

<a id='ref-97'></a>
### [97]
Seif Ismail, Antonio Arbues, Ryan Cotterell, Ren√© Zurbr√ºgg, and Carmen Amo Alonso. Narrate: Versatile language architecture for optimal control in robotics. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 9628‚Äì9635. IEEE, 2024.

<a id='ref-98'></a>
### [98]
Riccardo Andrea Izzo, Gianluca Bardaro, and Matteo Matteucci. Btgenbot: Behavior tree generation for robotic tasks with lightweight llms. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 9684‚Äì9690. IEEE, 2024.

<a id='ref-99'></a>
### [99]
Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Zhongxia Yan, and Cathy Wu. Intersectionzoo: Eco-driving for benchmarking multi-agent contextual reinforcement learning. arXiv preprint arXiv:2410.15221 , 2024.

<a id='ref-100'></a>
### [100]
Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, and Pascal Poupart. Reflect-then-plan: Offline model-based planning through a doubly bayesian lens. arXiv preprint arXiv:2506.06261 , 2025.

<a id='ref-101'></a>
### [101]
Ziqi Jia, Junjie Li, Xiaoyang Qu, and Jianzong Wang. Enhancing multi-agent systems via reinforcement learning with llm-based planner and graph-based policy. arXiv preprint arXiv:2503.10049 , 2025.

<a id='ref-102'></a>
### [102]
Wu Jialong, Yin Wenbiao, Jiang Yong, Wang Zhenglin, Xi Zekun, Fang Runnan, Zhang Linhai, He Yulan, Zhou Deyu, Xie Pengjun, and Huang Fei. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572v2 , 2025. URL https://www.arxiv.org/abs/2501.07572v2 .

<a id='ref-103'></a>
### [103]
Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. Self- planning code generation with large language models. ACM Transactions on Software Engineering and Methodology , 33(7):1‚Äì30, 2024.

<a id='ref-104'></a>
### [104]
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 , 2025.

<a id='ref-105'></a>
### [105]
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.09516 .

<a id='ref-106'></a>
### [106]
Koh Jing, Yu, Lo Robert, Jang Lawrence, Duvvur Vikram, Lim Ming, Chong, Huang Po-Yu, Neubig Graham, Zhou Shuyan, Salakhutdinov Ruslan, and Fried Daniel. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649 , 2024. URL https://www.arxiv.org/abs/2401. 13649 .

<a id='ref-107'></a>
### [107]
Wang Jize, Ma Zerun, Li Yining, Zhang Songyang, Chen Cailian, Chen Kai, and Le Xinyi. Gta: A benchmark for general tool agents. arXiv preprint arXiv:2407.08713 , 2024. URL https://www.arxiv.org/abs/2407. 08713 .

<a id='ref-108'></a>
### [108]
Laurent Jon, M., Janizek Joseph, D., Ruzo Michael, Hinks Michaela, M., Hammerling Michael, J., Narayanan Siddharth, Ponnapati Manvitha, White Andrew, D., and Rodriques Samuel, G. Lab-bench: Measuring capabilities of language models for biology research. arXiv preprint arXiv:2407.10362 , 2024. URL https://www.arxiv. org/abs/2407.10362 .

<a id='ref-109'></a>
### [109]
Hyun Jonathan, Waytowich Nicholas, R, and Chen Boyuan. Crew-wildfire: Benchmarking agentic multi-agent collaborations at scale. arXiv preprint arXiv:2507.05178v1 , 2025. URL https://www.arxiv.org/abs/2507. 05178v1 . 25 APREPRINT -

<a id='ref-110'></a>
### [110]
Light Jonathan, Cai Min, Shen Sheng, and Hu Ziniu. Avalonbench: Evaluating llms playing the game of avalon. arXiv preprint arXiv:2310.05036 , 2023. URL https://www.arxiv.org/abs/2310.05036 .

<a id='ref-111'></a>
### [111]
Singh Joykirat, Magazine Raghav, Pandya Yash, and Nambi Akshay. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441v1 , 2025. URL https://www.arxiv.org/ abs/2505.01441v1 .

<a id='ref-112'></a>
### [112]
Wu Junde, Zhu Jiayuan, and Liu Yuyuan. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644 , 2025. URL https://www.arxiv.org/abs/2502.04644 .

<a id='ref-113'></a>
### [113]
Ma Kaixin, Zhang Hongming, Wang Hongwei, Pan Xiaoman, Yu Wenhao, and Yu Dong. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172 , 2023. URL https://www. arxiv.org/abs/2309.08172 .

<a id='ref-114'></a>
### [114]
Chenglong Kang, Xiaoyi Liu, and Fei Guo. Retrointext: A multimodal large language model enhanced framework for retrosynthetic planning via in-context representation learning. In The Thirteenth International Conference on Learning Representations , 2025.

<a id='ref-115'></a>
### [115]
Thakrar Karishma, Basavatia Shreyas, and Daftardar Akshay. Cultivating multimodal intelligence: Interpretive reasoning and agentic rag approaches to dermatological diagnosis. arXiv preprint arXiv:2507.05520v1 , 2025. URL https://www.arxiv.org/abs/2507.05520v1 .

<a id='ref-116'></a>
### [116]
Valmeekam Karthik, Marquez Matthew, Olmo Alberto, Sreedharan Sarath, and Kambhampati Subbarao. Plan- bench: An extensible benchmark for evaluating large language models on planning and reasoning about change. arXiv preprint arXiv:2206.10498v4 , 2022. URL https://www.arxiv.org/abs/2206.10498v4 .

<a id='ref-117'></a>
### [117]
Michael Katz, Harsha Kokel, Kavitha Srinivas, and Shirin Sohrabi Araghi. Thought of search: Planning with language models through the lens of efficiency. Advances in Neural Information Processing Systems , 37: 138491‚Äì138568, 2024.

<a id='ref-118'></a>
### [118]
Basu Kinjal, Abdelaziz Ibrahim, Bradford Kelsey, Crouse Maxwell, Kate Kiran, Kumaravel Sadhana, Goyal Saurabh, Munawar Asim, Rizk Yara, Wang Xin, Lastras Luis, and Kapanipathi Pavan. Nestful: A benchmark for evaluating llms on nested sequences of api calls. arXiv preprint arXiv:2409.03797 , 2024. URL https: //www.arxiv.org/abs/2409.03797 .

<a id='ref-119'></a>
### [119]
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452 , 2023.

<a id='ref-120'></a>
### [120]
Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476 , 2024.

<a id='ref-121'></a>
### [121]
Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui Xie, and Ying Nian Wu. Latent plan transformer for trajectory abstraction: Planning as latent space inference. Advances in Neural Information Processing Systems , 37:123379‚Äì123401, 2024.

<a id='ref-122'></a>
### [122]
Dong-Ho Lee, Adyasha Maharana, Jay Pujara, Xiang Ren, and Francesco Barbieri. Realtalk: A 21-day real-world dataset for long-term conversation. arXiv preprint arXiv:2502.13270 , 2025.

<a id='ref-123'></a>
### [123]
Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N Ioannidis, Huzefa Rangwala, and Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation.

<a id='ref-124'></a>
### [124]
Joel Z Leibo, Edgar A Due√±ez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement learning with melting pot. In International conference on machine learning , pages 6187‚Äì6199. PMLR, 2021.

<a id='ref-125'></a>
### [125]
Pascal Leroy, Pablo G Morato, Jonathan Pisane, Athanasios Kolios, and Damien Ernst. Imp-marl: a suite of environments for large-scale infrastructure management planning via marl. Advances in neural information processing systems , 36:53522‚Äì53551, 2023.

<a id='ref-126'></a>
### [126]
Elad Levi and Ilan Kadar. Intellagent: A multi-agent framework for evaluating conversational ai systems. arXiv preprint arXiv:2501.11067 , 2025.

<a id='ref-127'></a>
### [127]
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval- augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Pro- cessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html . 26 APREPRINT -

<a id='ref-128'></a>
### [128]
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in neural information processing systems , 33:9459‚Äì9474, 2020.

<a id='ref-129'></a>
### [129]
Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, and Yaliang Li. Agent-oriented planning in multi-agent systems. arXiv preprint arXiv:2410.02189 , 2024.

<a id='ref-130'></a>
### [130]
Boyi Li, Zhonghan Zhao, Der-Horng Lee, and Gaoang Wang. Adaptive graph pruning for multi-agent communi- cation. arXiv preprint arXiv:2506.02951 , 2025.

<a id='ref-131'></a>
### [131]
Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, and Jiayi Shen. Smoa: Improving multi-agent large language models with sparse mixture-of-agents, 2024. URL https://arxiv.org/abs/2411. 03284 .

<a id='ref-132'></a>
### [132]
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 , pages 3102‚Äì3116. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.187. URL https://doi.org/10.18653/v1/ 2023.emnlp-main.187 .

<a id='ref-133'></a>
### [133]
Ting-Wei Li, Ruizhong Qiu, and Hanghang Tong. Model-free graph data selection under distribution shift, 2025.

<a id='ref-134'></a>
### [134]
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366 , 2025.

<a id='ref-135'></a>
### [135]
Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S Yu, Fei Huang, et al. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937 , 2024.

<a id='ref-136'></a>
### [136]
Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. ArXiv , abs/2407.12821, 2024. URL https://api.semanticscholar.org/CorpusID:271270428 .

<a id='ref-137'></a>
### [137]
Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Ke-Rong Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhiyang Xu, and Feiyu Xiong. Memos: An operating system for memory-augmented generation (mag) in large language models. ArXiv , abs/2505.22101, 2025. URL https://api.semanticscholar.org/CorpusID:278960153 .

<a id='ref-138'></a>
### [138]
Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Zhi-Quan Luo, and Ruoyu Sun. Preserving diversity in supervised fine-tuning of large language models. arXiv preprint arXiv:2408.16673 , 2024.

<a id='ref-139'></a>
### [139]
Xinnian Liang, Bing Wang, Huijia Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Scm: Enhancing large language model with self-controlled memory framework. 2023. URL https://api. semanticscholar.org/CorpusID:258331553 .

<a id='ref-140'></a>
### [140]
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis. CoRR , abs/2303.16434, 2023. doi: 10.48550/ARXIV .2303.16434. URL https://doi.org/10.48550/arXiv.2303.16434 .

<a id='ref-141'></a>
### [141]
Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B Tenenbaum, Tom Silver, Jo√£o F Henriques, and Kevin Ellis. Visualpredicator: Learning abstract world models with neuro-symbolic predicates for robot planning. arXiv preprint arXiv:2410.23156 , 2024.

<a id='ref-142'></a>
### [142]
Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the game of avalon, 2023. URL https://arxiv. org/abs/2310.05036 .

<a id='ref-143'></a>
### [143]
Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, and Joseph Gonzalez. Sleep- time compute: Beyond inference scaling at test-time. ArXiv , abs/2504.13171, 2025. URL https://api. semanticscholar.org/CorpusID:277857467 .

<a id='ref-144'></a>
### [144]
Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 19498‚Äì19508, 2025.

<a id='ref-145'></a>
### [145]
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations , 2023. 27 APREPRINT -

<a id='ref-146'></a>
### [146]
Xiao Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, and Hanghang Tong. BackTime: Backdoor attacks on multivariate time series forecasting. In Advances in Neural Information Processing Systems , volume 37, 2024.

<a id='ref-147'></a>
### [147]
Xiao Lin, Zhining Liu, Ze Yang, Gaotang Li, Ruizhong Qiu, Shuke Wang, Hui Liu, Haotian Li, Sumit Keswani, Vishwa Pardeshi, et al. Moralise: A structured benchmark for moral alignment in visual language models, 2025.

<a id='ref-148'></a>
### [148]
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. LLM+P: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477 , 2023.

<a id='ref-149'></a>
### [149]
Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, and Jie Chen. Multimodal large language models for inverse molecular design with retrosynthetic planning. arXiv preprint arXiv:2410.04223 , 2024.

<a id='ref-150'></a>
### [150]
Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .

<a id='ref-151'></a>
### [151]
Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, and Hanghang Tong. Logic query of thoughts: Guiding large language models to answer complex logic queries with knowledge graphs, 2024.

<a id='ref-152'></a>
### [152]
Shuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652 , 2025.

<a id='ref-153'></a>
### [153]
Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. arXiv preprint arXiv:2310.14628 , 2023.

<a id='ref-154'></a>
### [154]
Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, and Tianyu Du. Tool-planner: Task planning with clusters across multiple tools. arXiv preprint arXiv:2406.03807 , 2024.

<a id='ref-155'></a>
### [155]
Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, and Tianyu Du. Tool-planner: Task planning with clusters across multiple tools. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id= dRz3cizftU .

<a id='ref-156'></a>
### [156]
Zhining Liu, Zhichen Zeng, Ruizhong Qiu, Hyunsik Yoo, David Zhou, Zhe Xu, Yada Zhu, Kommy Weldemariam, Jingrui He, and Hanghang Tong. Topological augmentation for class-imbalanced node classification, 2023.

<a id='ref-157'></a>
### [157]
Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Hyunsik Yoo, David Zhou, Zhe Xu, Yada Zhu, Kommy Weldemariam, Jingrui He, and Hanghang Tong. Class-imbalanced graph learning without class rebalancing. In Proceedings of the 41st International Conference on Machine Learning , 2024.

<a id='ref-158'></a>
### [158]
Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Yada Zhu, Hendrik Hamann, and Hanghang Tong. AIM: Attributing, interpreting, mitigating data unfairness. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 2014‚Äì2025, 2024.

<a id='ref-159'></a>
### [159]
Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Yada Zhu, Hendrik Hamann, Jingrui He, and Hanghang Tong. Breaking silos: Adaptive model fusion unlocks better time series forecasting. In Proceedings of the 42nd International Conference on Machine Learning , 2025.

<a id='ref-160'></a>
### [160]
Zichen Liu, Guoji Fu, Chao Du, Wee Sun Lee, and Min Lin. Continual reinforcement learning by planning with online world models. arXiv preprint arXiv:2507.09177 , 2025.

<a id='ref-161'></a>
### [161]
Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott M Jordan, Adam White, Gabor Mihucz, Farzane Aminmansour, and Martha White. Goal-space planning with subgoal models. Journal of Machine Learning Research , 25(330):1‚Äì57, 2024.

<a id='ref-162'></a>
### [162]
Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291 , 2023.

<a id='ref-163'></a>
### [163]
Qian Long, Zhi Li, Ran Gong, Ying Nian Wu, Demetri Terzopoulos, and Xiaofeng Gao. Teamcraft: A benchmark for multi-modal multi-agent systems in minecraft. arXiv preprint arXiv:2412.05255 , 2024.

<a id='ref-164'></a>
### [164]
Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B Mazomenos, and Yueming Jin. Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence. arXiv preprint arXiv:2503.10265 , 2025.

<a id='ref-165'></a>
### [165]
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems , 36:43447‚Äì43478, 2023.

<a id='ref-166'></a>
### [166]
Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework with extensible tools for complex reasoning, 2025. URL https://arxiv.org/abs/2502.11271 . 28 APREPRINT -

<a id='ref-167'></a>
### [167]
Yining Lu, Haoping Yu, and Daniel Khashabi. GEAR: Augmenting language models with generalizable and efficient tool resolution. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 112‚Äì138, St. Julian‚Äôs, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.eacl-long.7. URL https://aclanthology.org/2024.eacl-long.7/ .

<a id='ref-168'></a>
### [168]
Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, and Giovanni Campagna. Wilbur: Adaptive in-context learning for robust and accurate web agents. arXiv preprint arXiv:2404.05902 , 2024.

<a id='ref-169'></a>
### [169]
Max Ruiz Luyten, Antonin Berthon, and Mihaela van der Schaar. Strategic planning: A top-down approach to option generation. In Forty-second International Conference on Machine Learning , 2025.

<a id='ref-170'></a>
### [170]
Artem Lykov and Dzmitry Tsetserukou. Llm-brain: Ai-driven fast generation of robot behaviour tree based on large language model. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM) , pages 392‚Äì397. IEEE, 2024.

<a id='ref-171'></a>
### [171]
Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195 , 2024.

<a id='ref-172'></a>
### [172]
Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems , 37:15497‚Äì15525, 2024.

<a id='ref-173'></a>
### [173]
Zhiyuan Ma, Jiayu Liu, Xianzhen Luo, Zhenya Huang, Qingfu Zhu, and Wanxiang Che. Advancing tool- augmented large language models via meta-verification and reflection learning. CoRR , abs/2506.04625, 2025. doi: 10.48550/ARXIV .2506.04625. URL https://doi.org/10.48550/arXiv.2506.04625 .

<a id='ref-174'></a>
### [174]
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534‚Äì46594, 2023.

<a id='ref-175'></a>
### [175]
Adyasha Maharana, Dong-Ho Lee, S. Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. ArXiv , abs/2402.17753, 2024. URL https://api. semanticscholar.org/CorpusID:268041615 .

<a id='ref-176'></a>
### [176]
Sadegh Mahdavi, Raquel Aoki, Keyi Tang, and Yanshuai Cao. Leveraging environment interaction for automated pddl translation and planning with large language models. Advances in Neural Information Processing Systems , 37:38960‚Äì39008, 2024.

<a id='ref-177'></a>
### [177]
Kelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo, Chenlong Deng, and Zhicheng Dou. Rag-studio: Towards in-domain adaptation of retrieval augmented generation through self-alignment. In Findings of the Association for Computational Linguistics: EMNLP 2024 , pages 725‚Äì735, 2024.

<a id='ref-178'></a>
### [178]
Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, and Aram Galstyan. Tree-of-traversals: A zero-shot reasoning algorithm for augmenting black-box language models with knowledge graphs. arXiv preprint arXiv:2407.21358 , 2024.

<a id='ref-179'></a>
### [179]
Sharath Matada, Luke Bhan, Yuanyuan Shi, and Nikolay Atanasov. Generalizable motion planning via operator learning. arXiv preprint arXiv:2410.17547 , 2024.

<a id='ref-180'></a>
### [180]
Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, and Kai-Wei Chang. Llm-a*: Large language model enhanced incremental heuristic search on path planning. arXiv preprint arXiv:2407.02511 , 2024.

<a id='ref-181'></a>
### [181]
Yue Meng and Chuchu Fan. Telograf: Temporal logic planning via graph-encoded flow matching. arXiv preprint arXiv:2505.00562 , 2025.

<a id='ref-182'></a>
### [182]
Zhuge Mingchen, Zhao Changsheng, Ashley Dylan, Wang Wenyi, Khizbullin Dmitrii, Xiong Yunyang, Liu Zechun, Chang Ernie, Krishnamoorthi Raghuraman, Tian Yuandong, Shi Yangyang, Chandra Vikas, and Schmidhuber J√ºrgen. Agent-as-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934 , 2024. URL https://www.arxiv.org/abs/2410.10934 .

<a id='ref-183'></a>
### [183]
Xing Mingzhe, Zhang Rongkai, Xue Hui, Chen Qi, Yang Fan, and Xiao Zhen. Understanding the weakness of large language model agents within a complex android environment. arXiv preprint arXiv:2402.06596v1 , 2024. URL https://www.arxiv.org/abs/2402.06596v1 .

<a id='ref-184'></a>
### [184]
Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch√ºtze. Ret-llm: Towards a general read-write memory for large language models. ArXiv , abs/2305.14322, 2023. URL https://api.semanticscholar. org/CorpusID:258841042 .

<a id='ref-185'></a>
### [185]
Shridhar Mohit, Yuan Xingdi, C√¥t√© Marc-Alexandre, Bisk Yonatan, Trischler Adam, and Hausknecht Matthew. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768 , 2020. URL https://www.arxiv.org/abs/2010.03768 . 29 APREPRINT -

<a id='ref-186'></a>
### [186]
Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare diplomacy: Benchmarking language model cooperation. arXiv preprint arXiv:2310.08901 , 2023.

<a id='ref-187'></a>
### [187]
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.

<a id='ref-188'></a>
### [188]
Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering , pages 1‚Äì13, 2024.

<a id='ref-189'></a>
### [189]
Sid Nayak, Adelmo Morrison Orozco, Marina Have, Jackson Zhang, Vittal Thirumalai, Darren Chen, Aditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, James Harrison, et al. Long-horizon planning for multi- agent robots in partially observable environments. Advances in Neural Information Processing Systems , 37: 67929‚Äì67967, 2024.

<a id='ref-190'></a>
### [190]
Minh Huynh Nguyen, Thang Phan Chau, Phong X Nguyen, and Nghi DQ Bui. Agilecoder: Dynamic collaborative agents for software development based on agile methodology. In 2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge) , pages 156‚Äì167. IEEE, 2025.

<a id='ref-191'></a>
### [191]
Thang Nguyen, Peter Chin, and Yu-Wing Tai. Ma-rag: Multi-agent retrieval-augmented generation via collabora- tive chain-of-thought reasoning, 2025. URL https://arxiv.org/abs/2505.20096 .

<a id='ref-192'></a>
### [192]
Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty. Sfr-rag: Towards contextually faithful llms. arXiv preprint arXiv:2409.09916 , 2024.

<a id='ref-193'></a>
### [193]
Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Yifu Yuan, Zibin Dong, Jinyi Liu, MingZhi Li, Yuzheng Zhuang, and Yan Zheng. Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation. Advances in Neural Information Processing Systems , 37:17541‚Äì17571, 2024.

<a id='ref-194'></a>
### [194]
Ruiqi Ni, Zherong Pan, and Ahmed H Qureshi. Physics-informed temporal difference metric learning for robot motion planning. arXiv preprint arXiv:2505.05691 , 2025.

<a id='ref-195'></a>
### [195]
Ziyi Ni, Yifan Li, and Daxiang Dong. Tree-of-code: A hybrid approach for robust complex task planning and execution. arXiv preprint arXiv:2412.14212 , 2024.

<a id='ref-196'></a>
### [196]
Shinn Noah, Cassano Federico, Berman Edward, Gopinath Ashwin, Narasimhan Karthik, and Yao Shunyu. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366 , 2023. URL https://www.arxiv.org/abs/2303.11366 .

<a id='ref-197'></a>
### [197]
Bard Nolan, Foerster Jakob, N., Chandar Sarath, Burch Neil, Lanctot Marc, Song H., Francis, Parisotto Emilio, Dumoulin Vincent, Moitra Subhodeep, Hughes Edward, Dunning Iain, Mourad Shibl, Larochelle Hugo, Bellemare Marc, G., and Bowling Michael. The hanabi challenge: A new frontier for ai research. arXiv preprint arXiv:1902.00506 , 2019. URL https://www.arxiv.org/abs/1902.00506 .

<a id='ref-198'></a>
### [198]
Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu. Comma: A communicative multimodal multi-agent benchmark. arXiv preprint arXiv:2410.07553 , 2024.

<a id='ref-199'></a>
### [199]
Siru Ouyang, Wenhao Yu, Kaixin Ma, Zi-Qiang Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. ArXiv , abs/2410.14684, 2024. URL https://api.semanticscholar.org/CorpusID:273502041 .

<a id='ref-200'></a>
### [200]
Laura O‚ÄôMahony, Leo Grinsztajn, Hailey Schoelkopf, and Stella Biderman. Attributing mode collapse in the fine-tuning of large language models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models , volume 2, 2024.

<a id='ref-201'></a>
### [201]
Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. ArXiv , abs/2310.08560, 2023. URL https://api.semanticscholar. org/CorpusID:263909014 .

<a id='ref-202'></a>
### [202]
Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139 , 2024.

<a id='ref-203'></a>
### [203]
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models, 2023. URL https: //arxiv.org/abs/2303.09014 .

<a id='ref-204'></a>
### [204]
Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning. arXiv preprint arXiv:2502.18439 , 2025. 30 APREPRINT -

<a id='ref-205'></a>
### [205]
Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. ArXiv , abs/2405.20309, 2024. URL https://api.semanticscholar.org/CorpusID:270122967 .

<a id='ref-206'></a>
### [206]
Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model con- nected with massive apis. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Pa- quet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ e4c61f578ff07830f5c37378dd3ecb0d-Abstract-Conference.html .

<a id='ref-207'></a>
### [207]
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems , 37:126544‚Äì126565, 2024.

<a id='ref-208'></a>
### [208]
Zhou Peilin, Leon Bruce, Ying Xiang, Zhang Can, Shao Yifan, Ye Qichen, Chong Dading, Jin Zhiling, Xie Chenxuan, Cao Meng, Gu Yuxin, Hong Sixin, Ren Jing, Chen Jian, Liu Chao, and Hua Yining. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314 , 2025. URL https://www.arxiv.org/abs/2504.19314 .

<a id='ref-209'></a>
### [209]
Qiu Pengcheng, Wu Chaoyi, Liu Shuyu, Zhao Weike, Chen Zhuoxia, Gu Hongfei, Peng Chuanjin, Zhang Ya, Wang Yanfeng, and Xie Weidi. Quantifying the reasoning abilities of llms on real-world clinical cases. arXiv preprint arXiv:2503.04691 , 2025. URL https://www.arxiv.org/abs/2503.04691 .

<a id='ref-210'></a>
### [210]
Jansen Peter, C√¥t√© Marc-Alexandre, Khot Tushar, Bransom Erin, Mishra Bhavana, Dalvi, Majumder Bodhisattwa, Prasad, Tafjord Oyvind, and Clark Peter. Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. arXiv preprint arXiv:2406.06769 , 2024. URL https://www.arxiv. org/abs/2406.06769 .

<a id='ref-211'></a>
### [211]
Nearchos Potamitis and Akhil Arora. Are retrials all you need? enhancing large language model reasoning without verbalized feedback. arXiv preprint arXiv:2504.12951 , 2025.

<a id='ref-212'></a>
### [212]
Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772 , 2023.

<a id='ref-213'></a>
### [213]
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 , 2022.

<a id='ref-214'></a>
### [214]
Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199 , 2024.

<a id='ref-215'></a>
### [215]
Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 6922‚Äì6939, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.462. URL https://aclanthology.org/2023.findings-emnlp.462/ .

<a id='ref-216'></a>
### [216]
Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T√ºr, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958 , 2025.

<a id='ref-217'></a>
### [217]
Haofu Qian, Chenjia Bai, Jiatao Zhang, Fei Wu, Wei Song, and Xuelong Li. Discriminator-guided embodied planning for llm agent. In The Thirteenth International Conference on Learning Representations , 2025.

<a id='ref-218'></a>
### [218]
Huang Qian, V ora Jian, Liang Percy, and Leskovec Jure. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302 , 2023. URL https://www.arxiv.org/abs/2310. 03302 .

<a id='ref-219'></a>
### [219]
Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems , 37:114843‚Äì114871, 2024.

<a id='ref-220'></a>
### [220]
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 , 2023.

<a id='ref-221'></a>
### [221]
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr . 31 APREPRINT -

<a id='ref-222'></a>
### [222]
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, Chi Han, Yi R. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Guoliang Li, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ACM Comput. Surv. , 57(4):101:1‚Äì101:40, 2025. doi: 10.1145/3704435. URL https://doi.org/10.1145/3704435 .

<a id='ref-223'></a>
### [223]
Ruizhong Qiu and Hanghang Tong. Gradient compressed sensing: A query-efficient gradient estimator for high-dimensional zeroth-order optimization. In Proceedings of the 41st International Conference on Machine Learning , 2024.

<a id='ref-224'></a>
### [224]
Ruizhong Qiu, Zhiqing Sun, and Yiming Yang. DIMES: A differentiable meta solver for combinatorial optimization problems. In Advances in Neural Information Processing Systems , volume 35, pages 25531‚Äì25546, 2022.

<a id='ref-225'></a>
### [225]
Ruizhong Qiu, Dingsu Wang, Lei Ying, H Vincent Poor, Yifang Zhang, and Hanghang Tong. Reconstructing graph diffusion history from a single snapshot. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 1978‚Äì1988, 2023.

<a id='ref-226'></a>
### [226]
Ruizhong Qiu, Jun-Gi Jang, Xiao Lin, Lihui Liu, and Hanghang Tong. TUCKET: A tensor time series data structure for efficient and accurate factor analysis over time ranges. Proceedings of the VLDB Endowment , 17 (13), 2024.

<a id='ref-227'></a>
### [227]
Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, and Hanghang Tong. Saffron-1: Safety inference scaling, 2025.

<a id='ref-228'></a>
### [228]
Ruizhong Qiu, Zhe Xu, Wenxuan Bao, and Hanghang Tong. Ask, and it shall be given: On the Turing completeness of prompting. In 13th International Conference on Learning Representations , 2025.

<a id='ref-229'></a>
### [229]
Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, and Christopher Lott. How efficient is LLM-generated code? A rigorous & high-standard benchmark. In 13th International Conference on Learning Representations , 2025.

<a id='ref-230'></a>
### [230]
Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: A survey. Frontiers of Computer Science , 19(8):198343, 2025.

<a id='ref-231'></a>
### [231]
Asif Rahman, Veljko Cvetkovic, Kathleen Reece, Aidan Walters, Yasir Hassan, Aneesh Tummeti, Bryan Torres, Denise Cooney, Margaret Ellis, and Dimitrios S Nikolopoulos. Marco: A multi-agent system for optimizing hpc code generation using large language models. arXiv preprint arXiv:2505.03906 , 2025.

<a id='ref-232'></a>
### [232]
Gong Ran, Huang Qiuyuan, Ma Xiaojian, V o Hoi, Durante Zane, Noda Yusuke, Zheng Zilong, Zhu Song-Chun, Terzopoulos Demetri, Fei-Fei Li, and Gao Jianfeng. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971 , 2023. URL https://www.arxiv.org/abs/2309.09971 .

<a id='ref-233'></a>
### [233]
Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D. Reid, and Niko S√ºnderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA , volume 229 of Proceedings of Machine Learning Research , pages 23‚Äì72. PMLR, 2023. URL https://proceedings.mlr.press/v229/rana23a.html .

<a id='ref-234'></a>
### [234]
Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: A temporal knowledge graph architecture for agent memory. ArXiv , abs/2501.13956, 2025. URL https://api.semanticscholar. org/CorpusID:275907122 .

<a id='ref-235'></a>
### [235]
Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, and Roberto Pieraccini. Pre-act: Multi-step planning and reasoning improves acting in llm agents. arXiv preprint arXiv:2505.09970 , 2025.

<a id='ref-236'></a>
### [236]
Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian Togelius, Kyunghyun Cho, and Joan Bruna. Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124 , 2018.

<a id='ref-237'></a>
### [237]
Xiao Ruixuan, Ma Wentao, Wang Ke, Wu Yuchuan, Zhao Junbo, Wang Haobo, Huang Fei, and Li Yongbin. Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents. arXiv preprint arXiv:2406.14884 , 2024. URL https://www.arxiv.org/abs/2406.14884 .

<a id='ref-238'></a>
### [238]
Anian Ruoss, Gr√©goire Del√©tang, Sourabh Medapati, Jordi Grau-Moya, Li K Wenliang, Elliot Catt, John Reid, Cannada A Lewis, Joel Veness, and Tim Genewein. Amortized planning with large-scale transformers: A case study on chess. Advances in Neural Information Processing Systems , 37:65765‚Äì65790, 2024. 32 APREPRINT -

<a id='ref-239'></a>
### [239]
Wang Ruoyao, Jansen Peter, C√¥t√© Marc-Alexandre, and Ammanabrolu Prithviraj. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540 , 2022. URL https://www.arxiv.org/abs/ 2203.07540 .

<a id='ref-240'></a>
### [240]
Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. System-1. x: Learning to balance fast and slow planning with language models. arXiv preprint arXiv:2407.14414 , 2024.

<a id='ref-241'></a>
### [241]
Schmidgall Samuel, Ziaei Rojin, Harris Carl, Reis Eduardo, Jopling Jeffrey, and Moor Michael. Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960 , 2024. URL https://www.arxiv.org/abs/2405.07960 .

<a id='ref-242'></a>
### [242]
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043 , 2019.

<a id='ref-243'></a>
### [243]
Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems , 36:68539‚Äì68551, 2023.

<a id='ref-244'></a>
### [244]
John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, et al. Mastering board games by external and internal planning with language models. arXiv preprint arXiv:2412.12119 , 2024.

<a id='ref-245'></a>
### [245]
Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379 , 2023.

<a id='ref-246'></a>
### [246]
Dhruv Shah, B≈Ça Àôzej Osi ¬¥nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on robot learning , pages 492‚Äì504. PMLR, 2023.

<a id='ref-247'></a>
### [247]
Chen Shan, Moreira Pedro, Xiao Yuxin, Schmidgall Sam, Warner Jeremy, Aerts Hugo, Hartvigsen Thomas, Gallifant Jack, and Bitterman Danielle, S. Medbrowsecomp: Benchmarking medical deep research and computer use. arXiv preprint arXiv:2505.14963 , 2025. URL https://www.arxiv.org/abs/2505.14963 .

<a id='ref-248'></a>
### [248]
Yixiang Shan, Zhengbang Zhu, Ting Long, Liang Qifan, Yi Chang, Weinan Zhang, and Liang Yin. Contradiff: Planning towards high return states via contrastive learning. In The Thirteenth International Conference on Learning Representations , 2025.

<a id='ref-249'></a>
### [249]
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems , 36: 38154‚Äì38180, 2023.

<a id='ref-250'></a>
### [250]
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023. URL https://arxiv.org/abs/2303.17580 .

<a id='ref-251'></a>
### [251]
Zhili Shen, Chenxin Diao, Pavlos V ougiouklis, Pascual Merita, Shriram Piramanayagam, Enting Chen, Damien Graux, Andre Melo, Ruofei Lai, Zeren Jiang, et al. Gear: Graph-enhanced agent for retrieval-augmented generation. arXiv preprint arXiv:2412.18431 , 2024.

<a id='ref-252'></a>
### [252]
Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive agents. arXiv preprint arXiv:2403.03031 , 2024.

<a id='ref-253'></a>
### [253]
Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic tool agents. In Proceedings of the ACM on Web Conference 2025 , pages 2222‚Äì2237, 2025.

<a id='ref-254'></a>
### [254]
Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, and Zhaochun Ren. Retrieval models aren‚Äôt tool-savvy: Benchmarking tool retrieval for large language models. CoRR , abs/2503.01763, 2025. doi: 10.48550/ARXIV .2503.01763. URL https://doi.org/10.48550/arXiv.2503. 01763 .

<a id='ref-255'></a>
### [255]
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems , 36:8634‚Äì8652, 2023.

<a id='ref-256'></a>
### [256]
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems , 2023. URL https://api.semanticscholar.org/CorpusID:258833055 . 33 APREPRINT -

<a id='ref-257'></a>
### [257]
Zhou Shuyan, Xu Frank, F., Zhu Hao, Zhou Xuhui, Lo Robert, Sridhar Abishek, Cheng Xianyi, Ou Tianyue, Bisk Yonatan, Fried Daniel, Alon Uri, and Neubig Graham. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 , 2023. URL https://www.arxiv.org/abs/2307. 13854 .

<a id='ref-258'></a>
### [258]
Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Agentic retrieval-augmented generation: A survey on agentic rag. arXiv preprint arXiv:2501.09136 , 2025.

<a id='ref-259'></a>
### [259]
Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, Edward Schmerling, and Marco Pavone. Real- time anomaly detection and reactive planning with large language models. arXiv preprint arXiv:2407.08735 , 2024.

<a id='ref-260'></a>
### [260]
Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399 , 2025.

<a id='ref-261'></a>
### [261]
Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin Yakovlev, and Aleksandr Panov. Pogema: A benchmark platform for cooperative multi-agent pathfinding. arXiv preprint arXiv:2407.14931 , 2024.

<a id='ref-262'></a>
### [262]
Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023 , pages 2986‚Äì2997. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00280. URL https://doi.org/10.1109/ICCV51070.2023.00280 .

<a id='ref-263'></a>
### [263]
Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624 , 2023.

<a id='ref-264'></a>
### [264]
Yuhang Song, Andrzej Wojcicki, Thomas Lukasiewicz, Jianyi Wang, Abi Aryan, Zhenghua Xu, Mai Xu, Zihan Ding, and Lianlong Wu. Arena: A general evaluation platform and building toolkit for multi-agent intelligence. InProceedings of the AAAI conference on artificial intelligence , volume 34, pages 7253‚Äì7260, 2020.

<a id='ref-265'></a>
### [265]
Henry W Sprueill, Carl Edwards, Mariefel V Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury. Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. arXiv preprint arXiv:2310.14420 , 2023.

<a id='ref-266'></a>
### [266]
Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588 , 2025.

<a id='ref-267'></a>
### [267]
Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. PEARL: prompting large language models to plan and execute actions over long documents. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julian‚Äôs, Malta, March 17-22, 2024 , pages 469‚Äì486. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.eacl-long.29 .

<a id='ref-268'></a>
### [268]
Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Yang Song, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1251‚Äì1261, 2025.

<a id='ref-269'></a>
### [269]
Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952 , 2025.

<a id='ref-270'></a>
### [270]
Ashraf Tajamul, Saqib Amal, Ghani Hanan, AlMahri Muhra, Li Yuhao, Ahsan Noor, Nawaz Umair, Lahoud Jean, Cholakkal Hisham, Shah Mubarak, Torr Philip, Khan Fahad, Shahbaz, Anwer Rao, Muhammad, and Khan Salman. Agent-x: Evaluating deep multimodal reasoning in vision-centric agentic tasks. arXiv preprint arXiv:2505.24876 , 2025. URL https://www.arxiv.org/abs/2505.24876 .

<a id='ref-271'></a>
### [271]
Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, and Zhenhua Dong. Membench: Towards more comprehensive evaluation on the memory of llm-based agents. arXiv preprint arXiv:2506.21605 , 2025.

<a id='ref-272'></a>
### [272]
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 , 2023.

<a id='ref-273'></a>
### [273]
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. CoRR , abs/2306.05301, 2023. doi: 10.48550/ARXIV . 2306.05301. URL https://doi.org/10.48550/arXiv.2306.05301 . 34 APREPRINT -

<a id='ref-274'></a>
### [274]
Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, and Mark Gerstein. Chemagent: Self-updating memories in large language models improves chemical reasoning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. URL https://openreview. net/forum?id=kuhIqeVg0e .

<a id='ref-275'></a>
### [275]
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 , 2025.

<a id='ref-276'></a>
### [276]
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 , 2025.

<a id='ref-277'></a>
### [277]
Thibaut Thonet, Jos Rozen, and Laurent Besacier. Elitr-bench: A meeting assistant benchmark for long-context language models. arXiv preprint arXiv:2403.20262 , 2024.

<a id='ref-278'></a>
### [278]
Xie Tianbao, Zhang Danyang, Chen Jixuan, Li Xiaochuan, Zhao Siheng, Cao Ruisheng, Hua Toh, Jing, Cheng Zhoujun, Shin Dongchan, Lei Fangyu, Liu Yitao, Xu Yiheng, Zhou Shuyan, Savarese Silvio, Xiong Caiming, Zhong Victor, and Yu Tao. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972v2 , 2024. URL https://www.arxiv.org/abs/ 2404.07972v2 .

<a id='ref-279'></a>
### [279]
Yuan Tongxin, He Zhiwei, Dong Lingzhong, Wang Yiming, Zhao Ruijie, Xia Tian, Xu Lizhen, Zhou Binglin, Li Fangqi, Zhang Zhuosheng, Wang Rui, and Liu Gongshen. R-judge: Benchmarking safety risk awareness for llm agents. arXiv preprint arXiv:2401.10019 , 2024. URL https://www.arxiv.org/abs/2401.10019 .

<a id='ref-280'></a>
### [280]
Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. Automl-agent: A multi-agent llm framework for full-pipeline automl. arXiv preprint arXiv:2410.02958 , 2024.

<a id='ref-281'></a>
### [281]
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 , 2022.

<a id='ref-282'></a>
### [282]
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain- of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 10014‚Äì10037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URL https://aclanthology.org/2023.acl-long. 557/ .

<a id='ref-283'></a>
### [283]
Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models: A critical investigation. Advances in Neural Information Processing Systems , 36:75993‚Äì76005, 2023.

<a id='ref-284'></a>
### [284]
Eugene Vinitsky, Nathan Lichtl√©, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. Advances in Neural Information Processing Systems , 35:3962‚Äì3974, 2022.

<a id='ref-285'></a>
### [285]
Kaustubh Vyas, Damien Graux, Yijun Yang, S√©bastien Montella, Chenxin Diao, Wendi Zhou, Pavlos V ougiouklis, Ruofei Lai, Yang Ren, Keshuang Li, et al. From an llm swarm to a pddl-empowered hive: Planning self-executed instructions in a multi-modal jungle. arXiv preprint arXiv:2412.12839 , 2024.

<a id='ref-286'></a>
### [286]
Luanbo Wan and Weizhi Ma. Storybench: A dynamic benchmark for evaluating long-term memory with multi turns. arXiv preprint arXiv:2506.13356 , 2025.

<a id='ref-287'></a>
### [287]
Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283 , 2024.

<a id='ref-288'></a>
### [288]
Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Networked time series imputation via position-aware graph enhanced variational autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 2256‚Äì2268, 2023.

<a id='ref-289'></a>
### [289]
Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, and Junshan Zhang. Adawm: Adaptive world model based planning for autonomous driving. arXiv preprint arXiv:2501.13072 , 2025.

<a id='ref-290'></a>
### [290]
Hongru Wang, Yujia Qin, Yankai Lin, Jeff Z Pan, and Kam-Fai Wong. Empowering large language models: Tool learning for real-world interaction. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 2983‚Äì2986, 2024. 35 APREPRINT -

<a id='ref-291'></a>
### [291]
Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. GTA: A bench- mark for general tool agents. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 8a75ee6d4b2eb0b777f549a32a5a5c28-Abstract-Datasets_and_Benchmarks_Track.html .

<a id='ref-292'></a>
### [292]
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and- solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091 , 2023.

<a id='ref-293'></a>
### [293]
Mianchu Wang, Rui Yang, Xi Chen, Hao Sun, Meng Fang, and Giovanni Montana. Goplan: Goal-conditioned offline reinforcement learning by planning with learned models. arXiv preprint arXiv:2310.20025 , 2023.

<a id='ref-294'></a>
### [294]
Pei Wang, Yanan Wu, Noah Wang, Jiaheng Liu, Xiaoshuai Song, Z. Y . Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng. Mtu- bench: A multi-granularity tool-use benchmark for large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. URL https://openreview.net/forum?id=6guG2OlXsr .

<a id='ref-295'></a>
### [295]
Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, and Jie Tang. Battleagentbench: A benchmark for evalu- ating cooperation and competition capabilities of language models in multi-agent systems. arXiv preprint arXiv:2408.15971 , 2024.

<a id='ref-296'></a>
### [296]
Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning , 2024.

<a id='ref-297'></a>
### [297]
Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian J. McAuley. MEMORYLLM: towards self-updatable large language models. InForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=p0lKWzdikQ .

<a id='ref-298'></a>
### [298]
Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rog√©rio Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory. ArXiv , abs/2502.00592, 2025. URL https://api.semanticscholar.org/CorpusID:276094378 .

<a id='ref-299'></a>
### [299]
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. CoRR , abs/2302.01560, 2023. doi: 10.48550/ARXIV .2302.01560. URL https://doi.org/10.48550/arXiv.2302.01560 .

<a id='ref-300'></a>
### [300]
Ziyan Wang, Zhicheng Zhang, Fei Fang, and Yali Du. M3hf: Multi-agent reinforcement learning from multi- phase human feedback of mixed quality. arXiv preprint arXiv:2503.02077 , 2025.

<a id='ref-301'></a>
### [301]
Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. ArXiv , abs/2409.07429, 2024. URL https://api.semanticscholar.org/CorpusID:272592995 .

<a id='ref-302'></a>
### [302]
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 24824‚Äì24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/ paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf .

<a id='ref-303'></a>
### [303]
Tianxin Wei, Ruizhong Qiu, Yifan Chen, Yunzhe Qi, Jiacheng Lin, Wenju Xu, Sreyashi Nag, Ruirui Li, Hanqing Lu, Zhengyang Wang, Chen Luo, Hui Liu, Suhang Wang, Jingrui He, Qi He, and Xianfeng Tang. Robust watermarking for diffusion models: A unified multi-dimensional recipe, 2024.

<a id='ref-304'></a>
### [304]
Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449 , 2025.

<a id='ref-305'></a>
### [305]
Tan Weihao, Jiang Changjiu, Duan Yu, Lei Mingcong, Li Jiageng, Hong Yitian, Wang Xinrun, and An Bo. Stardojo: Benchmarking open-ended behaviors of agentic multimodal llms in production-living simulations with stardew valley. arXiv preprint arXiv:2507.07445v2 , 2025. URL https://www.arxiv.org/abs/2507. 07445v2 .

<a id='ref-306'></a>
### [306]
Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, and Minlie Huang. Codeplan: Unlocking reasoning potential in large language models by scaling code-form planning. In The Thirteenth International Conference on Learning Representations , 2024. 36 APREPRINT -

<a id='ref-307'></a>
### [307]
Shi Wenqi, Xu Ran, Zhuang Yuchen, Yu Yue, Zhang Jieyu, Wu Hang, Zhu Yuanda, Ho Joyce, Yang Carl, and Wang May, D. Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records. arXiv preprint arXiv:2401.07128 , 2024. URL https://www.arxiv.org/abs/2401. 07128 .

<a id='ref-308'></a>
### [308]
Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Bench- marking chat assistants on long-term interactive memory. ArXiv , abs/2410.10813, 2024. URL https: //api.semanticscholar.org/CorpusID:273345961 .

<a id='ref-309'></a>
### [309]
Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813 , 2024.

<a id='ref-310'></a>
### [310]
Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644 , 2025.

<a id='ref-311'></a>
### [311]
Mengsong Wu, Tong Zhu, Han Han, Xiang Zhang, Wenbiao Shao, and Wenliang Chen. Chain-of-tools: Utilizing massive unseen tools in the cot reasoning of frozen language models. CoRR , abs/2503.16779, 2025. doi: 10.48550/ARXIV .2503.16779. URL https://doi.org/10.48550/arXiv.2503.16779 .

<a id='ref-312'></a>
### [312]
Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioanni- dis, Karthik Subbian, Jure Leskovec, and James Zou. Avatar: Optimizing llm agents for tool usage via contrastive reasoning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems , volume 37, pages 25981‚Äì26010. Cur- ran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 2db8ce969b000fe0b3fb172490c33ce8-Paper-Conference.pdf .

<a id='ref-313'></a>
### [313]
Ziwei Wu, Lecheng Zheng, Yuancheng Yu, Ruizhong Qiu, John Birge, and Jingrui He. Fair anomaly detection for imbalanced groups, 2024.

<a id='ref-314'></a>
### [314]
Georg W√∂lflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelovi ¬¥c, and Jakob Nikolas Kather. Llm agents making agent tools, 2025. URL https://arxiv.org/abs/2502.11705 .

<a id='ref-315'></a>
### [315]
Deng Xiang, Gu Yu, Zheng Boyuan, Chen Shijie, Stevens Samuel, Wang Boshi, Sun Huan, and Su Yu. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070 , 2023. URL https: //www.arxiv.org/abs/2306.06070 .

<a id='ref-316'></a>
### [316]
Tang Xiangru, Zou Anni, Zhang Zhuosheng, Li Ziming, Zhao Yilun, Zhang Xingyao, Cohan Arman, and Gerstein Mark. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537 , 2023. URL https://www.arxiv.org/abs/2311.10537 .

<a id='ref-317'></a>
### [317]
Tang Xiangru, Shao Daniel, Sohn Jiwoong, Chen Jiapeng, Zhang Jiayi, Xiang Jinyu, Wu Fang, Zhao Yilun, Wu Chenglin, Shi Wenqi, Cohan Arman, and Gerstein Mark. Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint arXiv:2503.07459 , 2025. URL https://www.arxiv.org/abs/2503.07459 .

<a id='ref-318'></a>
### [318]
Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, Ramin Hasani, Mathias Lechner, and Daniela Rus. Safediffuser: Safe planning with diffusion probabilistic models. In The Thirteenth International Conference on Learning Representations , 2023.

<a id='ref-319'></a>
### [319]
Li Xiaoxi, Dong Guanting, Jin Jiajie, Zhang Yuyao, Zhou Yujia, Zhu Yutao, Zhang Peitian, and Dou Zhicheng. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366 , 2025. URL https://www.arxiv.org/abs/2501.05366 .

<a id='ref-320'></a>
### [320]
Amber Xie, Oleh Rybkin, Dorsa Sadigh, and Chelsea Finn. Latent diffusion planning for imitation learning. arXiv preprint arXiv:2504.16925 , 2025.

<a id='ref-321'></a>
### [321]
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self- evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems , 36:41618‚Äì 41650, 2023.

<a id='ref-322'></a>
### [322]
L√π Xing, Han, Kasner Zden Àáek, and Reddy Siva. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930 , 2024. URL https://www.arxiv.org/abs/2402.05930 .

<a id='ref-323'></a>
### [323]
Wang Xingyao, Chen Yangyi, Yuan Lifan, Zhang Yizhe, Li Yunzhu, Peng Hao, and Ji Heng. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030 , 2024. URL https://www.arxiv.org/abs/ 2402.01030 .

<a id='ref-324'></a>
### [324]
Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. REWOO: De- coupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323 , 2023. 37 APREPRINT -

<a id='ref-325'></a>
### [325]
Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. arXiv preprint arXiv:2311.08562 , 2023.

<a id='ref-326'></a>
### [326]
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. CoRR , abs/2305.16504, 2023. doi: 10.48550/ARXIV .2305. 16504. URL https://doi.org/10.48550/arXiv.2305.16504 .

<a id='ref-327'></a>
### [327]
Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C Ho, Haoyu Wang, and Carl Yang. Collab-rag: Boosting retrieval-augmented generation for complex question answering via white-box and black-box llm collaboration. arXiv preprint arXiv:2504.04915 , 2025.

<a id='ref-328'></a>
### [328]
Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110 , 2025.

<a id='ref-329'></a>
### [329]
Zhe Xu, Ruizhong Qiu, Yuzhong Chen, Huiyuan Chen, Xiran Fan, Menghai Pan, Zhichen Zeng, Mahashweta Das, and Hanghang Tong. Discrete-state continuous-time diffusion for graph generation. In Advances in Neural Information Processing Systems , volume 37, 2024.

<a id='ref-330'></a>
### [330]
Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems , 37:113519‚Äì113544, 2024.

<a id='ref-331'></a>
### [331]
Ruihan Yang, Jiangjie Chen, Yikai Zhang, Siyu Yuan, Aili Chen, Kyle Richardson, Yanghua Xiao, and Deqing Yang. SELFGOAL: Your language agents already know how to achieve high-level goals. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 799‚Äì819, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176- 189-6. doi: 10.18653/v1/2025.naacl-long.36. URL https://aclanthology.org/2025.naacl-long.36/ .

<a id='ref-332'></a>
### [332]
Xianliang Yang, Zhihao Liu, Wei Jiang, Chuheng Zhang, Li Zhao, Lei Song, and Jiang Bian. A versatile multi-agent reinforcement learning benchmark for inventory management. arXiv preprint arXiv:2306.07542 , 2023.

<a id='ref-333'></a>
### [333]
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran Jiang, Ziyu Jiang, et al. Crag-comprehensive rag benchmark. Advances in Neural Information Processing Systems , 37:10470‚Äì10490, 2024.

<a id='ref-334'></a>
### [334]
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems , 36:11809‚Äì11822, 2023.

<a id='ref-335'></a>
### [335]
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR) , 2023.

<a id='ref-336'></a>
### [336]
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. Re- act: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X .

<a id='ref-337'></a>
### [337]
Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157 , 2024.

<a id='ref-338'></a>
### [338]
Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, and Jing Shao. Mas-gpt: Training llms to build llm-based multi-agent systems. arXiv preprint arXiv:2503.03686 , 2025.

<a id='ref-339'></a>
### [339]
Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Rational decision-making agent with internalized utility judgment. arXiv preprint arXiv:2308.12519 , 2023.

<a id='ref-340'></a>
### [340]
Pan Yichen, Kong Dehan, Zhou Sida, Cui Cheng, Leng Yifei, Jiang Bing, Liu Hangyu, Shang Yanyi, Zhou Shuyan, Wu Tongshuang, and Wu Zhengyang. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373v3 , 2024. URL https://www.arxiv.org/abs/2406.12373v3 .

<a id='ref-341'></a>
### [341]
Song Yifan, Xiong Weimin, Zhu Dawei, Wu Wenhao, Qian Han, Song Mingbo, Huang Hailiang, Li Cheng, Wang Ke, Yao Rong, Tian Ye, and Li Sujian. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624 , 2023. URL https://www.arxiv.org/abs/2306.06624 .

<a id='ref-342'></a>
### [342]
Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. arXiv preprint arXiv:2311.05657 , 2023. 38 APREPRINT -

<a id='ref-343'></a>
### [343]
Jiang Yixing, Black Kameron, C., Geng Gloria, Park Danny, Zou James, Ng Andrew, Y ., and Chen Jonathan, H. Medagentbench: A realistic virtual ehr environment to benchmark medical llm agents. arXiv preprint arXiv:2501.14654 , 2025. URL https://www.arxiv.org/abs/2501.14654 .

<a id='ref-344'></a>
### [344]
Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, and Hanghang Tong. Ensuring user-side fairness in dynamic recommender systems. In Proceedings of the ACM on Web Conference 2024 , pages 3667‚Äì3678, 2024.

<a id='ref-345'></a>
### [345]
Hyunsik Yoo, SeongKu Kang, Ruizhong Qiu, Charlie Xu, Fei Wang, and Hanghang Tong. Embracing plasticity: Balancing stability and plasticity in continual recommender systems. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2025.

<a id='ref-346'></a>
### [346]
Hyunsik Yoo, Ruizhong Qiu, Charlie Xu, Fei Wang, and Hanghang Tong. Generalizable recommender system during temporal popularity distribution shifts. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 2025.

<a id='ref-347'></a>
### [347]
Minjong Yoo, Jinwoo Jang, Wei-Jin Park, and Honguk Woo. Exploratory retrieval-augmented planning for continual embodied instruction following. Advances in Neural Information Processing Systems , 37:67034‚Äì67060, 2024.

<a id='ref-348'></a>
### [348]
Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, and Sungjin Ahn. Monte carlo tree diffusion for system 2 planning. arXiv preprint arXiv:2502.07202 , 2025.

<a id='ref-349'></a>
### [349]
Hong Qing Yu and Frank McQuade. Rag-kg-il: A multi-agent hybrid framework for reducing hallucinations and enhancing llm reasoning through rag and incremental knowledge graph learning integration, 2025. URL https://arxiv.org/abs/2503.13514 .

<a id='ref-350'></a>
### [350]
Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259 , 2025.

<a id='ref-351'></a>
### [351]
Xianhao Yu, Jiaqi Fu, Renjia Deng, and Wenjuan Han. Mineland: Simulating large-scale multi-agent interactions with limited multimodal senses and physical needs. arXiv preprint arXiv:2403.19267 , 2024.

<a id='ref-352'></a>
### [352]
Xiao Yu, Maximillian Chen, and Zhou Yu. Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning. arXiv preprint arXiv:2305.13660 , 2023.

<a id='ref-353'></a>
### [353]
Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W. Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced LLM trading agent with layered memory and character design. In Ron P. A. Petrick and Christopher W. Geib, editors, Proceedings of the AAAI 2024 Spring Symposium Series, Stanford, CA, USA, March 25-27, 2024 , pages 595‚Äì597. AAAI Press, 2024. doi: 10.1609/AAAISS.V3I1.31290. URL https://doi.org/10.1609/aaaiss.v3i1.31290 .

<a id='ref-354'></a>
### [354]
Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=G0vdDSt9XM .

<a id='ref-355'></a>
### [355]
Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201 , 2024.

<a id='ref-356'></a>
### [356]
Xi Yunjia, Lin Jianghao, Zhu Menghui, Xiao Yongzhao, Ou Zhuoying, Liu Jiaqi, Wan Tong, Chen Bo, Liu Weiwen, Wang Yasheng, Tang Ruiming, Zhang Weinan, and Yu Yong. Infodeepseek: Benchmarking agentic information seeking for retrieval-augmented generation. arXiv preprint arXiv:2505.15872 , 2025. URL https: //www.arxiv.org/abs/2505.15872 .

<a id='ref-357'></a>
### [357]
Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823 , 2023.

<a id='ref-358'></a>
### [358]
Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471 , 2025.

<a id='ref-359'></a>
### [359]
Xianghua Zeng, Hang Su, Zhengyi Wang, and Zhiyuan Lin. Graph diffusion for robust multi-agent coordination. InForty-second International Conference on Machine Learning .

<a id='ref-360'></a>
### [360]
Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and Ting Liu. Boosting tool use of large language models via iterative reinforced fine-tuning. arXiv e-prints , pages arXiv‚Äì2501, 2025. 39 APREPRINT -

<a id='ref-361'></a>
### [361]
Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Hybinette Balch, and Manuela Veloso. Flowmind: Automatic workflow generation with llms. Proceedings of the Fourth ACM International Conference on AI in Finance , 2023. URL https://api.semanticscholar.org/CorpusID: 265452485 .

<a id='ref-362'></a>
### [362]
Zhichen Zeng, Ruizhong Qiu, Zhe Xu, Zhining Liu, Yuchen Yan, Tianxin Wei, Lei Ying, Jingrui He, and Hanghang Tong. Graph mixup on approximate Gromov‚ÄìWasserstein geodesics. In Proceedings of the 41st International Conference on Machine Learning , 2024.

<a id='ref-363'></a>
### [363]
Zhichen Zeng, Ruizhong Qiu, Wenxuan Bao, Tianxin Wei, Xiao Lin, Yuchen Yan, Tarek F. Abdelzaher, Jiawei Han, and Hanghang Tong. Pave your own path: Graph gradual domain adaptation on fused Gromov‚ÄìWasserstein geodesics, 2025.

<a id='ref-364'></a>
### [364]
Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506 , 2024.

<a id='ref-365'></a>
### [365]
Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782 , 2024.

<a id='ref-366'></a>
### [366]
Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture search via agentic supernet. arXiv preprint arXiv:2502.04180 , 2025.

<a id='ref-367'></a>
### [367]
Han Zhang, Langshi Zhou, and Hanfang Yang. Learning to retrieve and reason on knowledge graph through active self-reflection. arXiv preprint arXiv:2502.14932 , 2025.

<a id='ref-368'></a>
### [368]
Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762 , 2024.

<a id='ref-369'></a>
### [369]
Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin Wu. Aflow: Automating agentic workflow generation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. URL https://openreview.net/forum?id= z5uVAKwmjf .

<a id='ref-370'></a>
### [370]
Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131 , 2024.

<a id='ref-371'></a>
### [371]
Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, 2025. URL https://arxiv.org/abs/2506.12508 .

<a id='ref-372'></a>
### [372]
Yaolun Zhang, Xiaogeng Liu, and Chaowei Xiao. Metaagent: Automatically constructing multi-agent systems based on finite state machines. arXiv preprint arXiv:2507.22606 , 2025.

<a id='ref-373'></a>
### [373]
Yiran Zhang, Mo Wang, Xiaoyang Li, Kaixuan Ren, Chencheng Zhu, and Usman Naseem. Turnbench- ms: A benchmark for evaluating multi-turn, multi-step reasoning in large language models. arXiv preprint arXiv:2506.01341 , 2025.

<a id='ref-374'></a>
### [374]
Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan O Arik. Chain of agents: Large language models collaborating on long-context tasks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. URL https://openreview.net/forum?id=LuCLf4BJsr .

<a id='ref-375'></a>
### [375]
Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants. arXiv preprint arXiv:2409.20163 , 2024.

<a id='ref-376'></a>
### [376]
Zijing Zhang, Zhanpeng Chen, He Zhu, Ziyang Chen, Nan Du, and Xiaolong Li. Toolexpnet: Optimizing multi-tool selection in llms with similarity and dependency-aware experience networks. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025 , pages 15706‚Äì15722. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.findings-acl.811/ .

<a id='ref-377'></a>
### [377]
Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844 , 2025.

<a id='ref-378'></a>
### [378]
Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling. CoRR , abs/2507.07998, 2025. doi: 10.48550/ARXIV .2507.07998. URL https://doi.org/10.48550/arXiv.2507.07998 . 40 APREPRINT -

<a id='ref-379'></a>
### [379]
Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in neural information processing systems , 36:31967‚Äì31987, 2023.

<a id='ref-380'></a>
### [380]
Xiang Zhen, Zheng Linzhi, Li Yanjie, Hong Junyuan, Li Qinbin, Xie Han, Zhang Jiawei, Xiong Zidi, Xie Chulin, Yang Carl, Song Dawn, and Li Bo. Guardagent: Safeguard llm agents by a guard agent via knowledge-enabled reasoning. arXiv preprint arXiv:2406.09187 , 2024. URL https://www.arxiv.org/abs/2406.09187 .

<a id='ref-381'></a>
### [381]
Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu. Magent: A many-agent reinforcement learning platform for artificial collective intelligence. In Proceedings of the AAAI conference on artificial intelligence , volume 32, 2018.

<a id='ref-382'></a>
### [382]
Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. Toolrerank: Adaptive and hierarchy- aware reranking for tool retrieval. In Nicoletta Calzolari, Min-Yen Kan, V√©ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computa- tional Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy , pages 16263‚Äì16273. ELRA and ICCL, 2024. URL https://aclanthology.org/2024.lrec-main.1413 .

<a id='ref-383'></a>
### [383]
Yupeng Zheng, Zebin Xing, Qichao Zhang, Bu Jin, Pengfei Li, Yuhang Zheng, Zhongpu Xia, Kun Zhan, Xianpeng Lang, Yaran Chen, and Dongbin Zhao. Planagent: A multi-modal large language agent for closed-loop vehicle motion planning. ArXiv , abs/2406.01587, 2024. URL https://api.semanticscholar.org/CorpusID: 270214282 .

<a id='ref-384'></a>
### [384]
Yupeng Zheng, Zebin Xing, Qichao Zhang, Bu Jin, Pengfei Li, Yuhang Zheng, Zhongpu Xia, Kun Zhan, Xianpeng Lang, Yaran Chen, et al. Planagent: A multi-modal large language agent for closed-loop vehicle motion planning. arXiv preprint arXiv:2406.01587 , 2024.

<a id='ref-385'></a>
### [385]
Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deep- researcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160 , 2025.

<a id='ref-386'></a>
### [386]
Liu Zhiwei, Yao Weiran, Zhang Jianguo, Xue Le, Heinecke Shelby, Murthy Rithesh, Feng Yihao, Chen Zeyuan, Niebles Juan, Carlos, Arpit Devansh, Xu Ran, Mui Phil, Wang Huan, Xiong Caiming, and Savarese Silvio. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960 , 2023. URL https://www.arxiv.org/abs/2308.05960 .

<a id='ref-387'></a>
### [387]
Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu, Jianye Hao, Mingxuan Yuan, and Junchi Yan. Flexplanner: Flexible 3d floorplanning via deep reinforcement learning in hybrid action space with multi-modality representation. Advances in Neural Information Processing Systems , 37:49252‚Äì49278, 2024.

<a id='ref-388'></a>
### [388]
Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. ArXiv , abs/2305.10250, 2023. URL https://api.semanticscholar.org/ CorpusID:258741194 .

<a id='ref-389'></a>
### [389]
Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625 , 2022.

<a id='ref-390'></a>
### [390]
Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983 , 2024.

<a id='ref-391'></a>
### [391]
Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vuli ¬¥c, Anna Korhonen, and Ser- can √ñ Arƒ±k. Multi-agent design: Optimizing agents with better prompts and topologies. arXiv preprint arXiv:2502.02533 , 2025.

<a id='ref-392'></a>
### [392]
Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving. arXiv preprint arXiv:2010.09776 , 2020.

<a id='ref-393'></a>
### [393]
Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478 , 2025.

<a id='ref-394'></a>
### [394]
Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841 , 2025.

<a id='ref-395'></a>
### [395]
Zikang Zhou, HU Haibo, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and Chun Jason Xue. Behaviorgpt: Smart agent simulation for autonomous driving with next-patch prediction. Advances in Neural Information Processing Systems , 37:79597‚Äì79617, 2024. 41 APREPRINT -

<a id='ref-396'></a>
### [396]
Guobin Zhu, Rui Zhou, Wenkang Ji, and Shiyu Zhao. Lamarl: Llm-aided multi-agent reinforcement learning for cooperative policy generation. IEEE Robotics and Automation Letters , 2025.

<a id='ref-397'></a>
### [397]
Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, et al. Multiagentbench: Evaluating the collaboration and competition of llm agents. arXiv preprint arXiv:2503.01935 , 2025.

<a id='ref-398'></a>
### [398]
Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng Dou. Inters: Unlocking the power of large language models in search with instruction tuning. arXiv preprint arXiv:2401.06532 , 2024.

<a id='ref-399'></a>
### [399]
Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for LLM question answering with external tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: An- nual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 9cb2a7495900f8b602cb10159246a016-Abstract-Datasets_and_Benchmarks.html .

<a id='ref-400'></a>
### [400]
Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor S. Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=B6pQxqUcT8 .

<a id='ref-401'></a>
### [401]
Wang Zilong, Cui Yuedong, Zhong Li, Zhang Zimin, Yin Da, Lin Bill, Yuchen, and Shang Jingbo. Of- ficebench: Benchmarking language agents across multiple applications for office automation. arXiv preprint arXiv:2407.19056 , 2024. URL https://www.arxiv.org/abs/2407.19056 .

<a id='ref-402'></a>
### [402]
Chen Ziru, Chen Shijie, Ning Yuting, Zhang Qianheng, Wang Boshi, Yu Botao, Li Yifei, Liao Zeyi, Wei Chen, Lu Zitong, Dey Vishal, Xue Mingyi, Baker Frazier, N., Burns Benjamin, Adu-Ampratwum Daniel, Huang Xuhui, Ning Xia, Gao Song, Su Yu, and Sun Huan. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080 , 2024. URL https: //www.arxiv.org/abs/2410.05080 .

<a id='ref-403'></a>
### [403]
Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning , pages 2165‚Äì2183. PMLR, 2023.

<a id='ref-404'></a>
### [404]
Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, and Jingrui He. Transformer copilot: Learning from the mistake log in LLM fine-tuning, 2025.

<a id='ref-405'></a>
### [405]
Jiaru Zou, Dongqi Fu, Sirui Chen, Xinrui He, Zihao Li, Yada Zhu, Jiawei Han, and Jingrui He. Gtr: Graph-table- rag for cross-table question answering. arXiv preprint arXiv:2504.01346 , 2025.

<a id='ref-406'></a>
### [406]
Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. arXiv preprint arXiv:2506.18896 , 2025.

<a id='ref-407'></a>
### [407]
Yunheng Zou, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge A. Campos Gonzalez Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian Zhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, and Al√°n Aspuru-Guzik. El agente: An autonomous agent for quantum chemistry. CoRR , abs/2505.02484, 2025. doi: 10.48550/ARXIV .2505.02484. URL https://doi.org/10. 48550/arXiv.2505.02484 . 42

</details>


